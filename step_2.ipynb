{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gab-palmeri/aml-geolocalization/blob/sam/testing_default.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The purpose of this notebook is to test the default model on three different datasets. The first is sf-xs test, the second is tokyo-xs test, and the last one is tokyo-night, a subset of the tokyo dataset with only night images as query images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf3z5SqWZ91b"
      },
      "source": [
        "# Torch "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YbtEmI1AiTkF",
        "outputId": "93ee7ec7-4878-4b54-8f18-aa09d56eb5d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faiss_cpu>=1.7.1\n",
            "  Downloading faiss_cpu-1.7.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.0 MB 15.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Pillow>=9.0.1\n",
            "  Downloading Pillow-9.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 13.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: Pillow\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "Successfully installed Pillow-9.3.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit_learn>=1.0.2 in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn>=1.0.2) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn>=1.0.2) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit_learn>=1.0.2) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit_learn>=1.0.2) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch>=1.8.2 in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.2) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision>=0.9.2 in /usr/local/lib/python3.8/dist-packages (0.14.0+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.9.2) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.9.2) (4.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.9.2) (9.3.0)\n",
            "Requirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.9.2) (1.13.0+cu116)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.9.2) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.9.2) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.9.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.9.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.9.2) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting utm>=0.7.0\n",
            "  Downloading utm-0.7.0.tar.gz (8.7 kB)\n",
            "Building wheels for collected packages: utm\n",
            "  Building wheel for utm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for utm: filename=utm-0.7.0-py3-none-any.whl size=6108 sha256=a7a47c19560a98f37480881e493541a6523396b2021c98ab58d1e0b235f64ec5\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/e2/d8/878a8cc986641056fbfebefc4d8eb64238a7b6d3426e86b447\n",
            "Successfully built utm\n",
            "Installing collected packages: utm\n",
            "Successfully installed utm-0.7.0\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "# CosPlace requirements\n",
        "!pip3 install \"faiss_cpu>=1.7.1\"\n",
        "!pip3 install \"numpy>=1.21.2\"\n",
        "!pip3 install \"Pillow>=9.0.1\"\n",
        "!pip3 install \"scikit_learn>=1.0.2\"\n",
        "!pip3 install \"torch>=1.8.2\"\n",
        "!pip3 install \"torchvision>=0.9.2\"\n",
        "!pip3 install \"tqdm>=4.62.3\"\n",
        "!pip3 install \"utm>=0.7.0\"\n",
        "\n",
        "import torch\n",
        "#use GPU if available \n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #'cpu' # 'cuda' or 'cpu'\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czjvnq3FjBmh"
      },
      "source": [
        "# Download Datasets and previous data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGOhXMNqjMed"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gdown\n",
        "\n",
        "def download(id, output=None, quiet=True):\n",
        "  gdown.download(\n",
        "    f\"https://drive.google.com/uc?export=download&confirm=pbef&id={id}\",\n",
        "    output=output,\n",
        "    quiet=quiet\n",
        "  )\n",
        "\n",
        "# TOKYO-XS DATASET\n",
        "if not os.path.isdir(\"/content/tokyo_xs\"):\n",
        "  id = \"1fBCnap5BRh36474cVkjvjlC-yUTEb1n3\"\n",
        "  download(id, quiet=False)                           # download from our gdrive\n",
        "  !jar xvf \"/content/tokyo-xs.zip\"                    # unzip\n",
        "  !rm -r \"/content/tokyo-xs.zip\"                      # remove .zip file\n",
        "\n",
        "# TOKYO NIGHT DATASET\n",
        "# if not os.path.isdir(\"/content/tokyo_night\"):\n",
        "# id = \"tokyo_night_id\"\n",
        "#  download(id, quiet=False)                           # download from our gdrive\n",
        "#  !jar xvf \"/content/tokyo-night.zip\"                    # unzip\n",
        "#  !rm -r \"/content/tokyo-night.zip\"  \n",
        "\n",
        "# SAN FRANCISCO DATASET\n",
        "if not os.path.isdir(\"/content/small\"):\n",
        "  id = \"1brIxBJmOgvuzFbI57f5LxnMxjccUu993\"\n",
        "  download(id, quiet=False)                           # download\n",
        "  !jar xvf \"/content/sf-xs.zip\"                       # unzip\n",
        "  !rm -r \"/content/sf-xs.zip\"                         # remove .zip file\n",
        "\n",
        "\n",
        "# CACHE\n",
        "if not os.path.isdir(\"/content/cache\"):\n",
        "  id = \"1J6InuF4I_OsiuwvVPRrwDfP-W2nz8Kn0\"\n",
        "  download(id, quiet=False)                           # download\n",
        "  !jar xvf \"/content/cache.zip\"                       # unzip\n",
        "  !cp -r \"/content/content/cache/.\" \"/content/cache/\" # copy in main dir\n",
        "  !rm -rf \"/content/content/\"                         # remove old dir\n",
        "  !rm -r \"/content/cache.zip\"                         # remove .zip file\n",
        "\n",
        "# LOGS\n",
        "if not os.path.isdir(\"/content/logs\"):\n",
        "  id = \"1q9x_SGDJxO3D-niFyOe5inkoO-8ONHcW\"\n",
        "  download(id, quiet=False)                           # download\n",
        "  !jar xvf \"/content/logs.zip\"                        # unzip\n",
        "  !cp -r \"/content/content/logs/.\" \"/content/logs/\"   # copy in main dir\n",
        "  !rm -rf \"/content/content/\"                         # remove old dir\n",
        "  !rm -r \"/content/logs.zip\"                          # remove .zip file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g6SkCgyhl-g"
      },
      "source": [
        "# Download Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63SgJ_Y0hwrC",
        "outputId": "fc7d04b5-6234-4895-ba40-51b9d4a1b348"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'CosPlace'...\n",
            "remote: Enumerating objects: 168, done.\u001b[K\n",
            "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 168 (delta 54), reused 53 (delta 32), pack-reused 85\u001b[K\n",
            "Receiving objects: 100% (168/168), 55.31 KiB | 11.06 MiB/s, done.\n",
            "Resolving deltas: 100% (84/84), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone \"https://github.com/gmberton/CosPlace\" \n",
        "#!rm -r \"/content/CosPlace\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiwWzjzSio-h"
      },
      "source": [
        "\n",
        "\n",
        "# Import Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qlBEtAcisfV"
      },
      "outputs": [],
      "source": [
        "# IMPORT COSPLACE PYTHON FUNCTIONS \n",
        "import sys, os\n",
        "\n",
        "sys.path.append(\"/content/CosPlace/\")\n",
        "\n",
        "import CosPlace\n",
        "from CosPlace import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# CosPlace Groups parameters\n",
        "COS_M = 10\n",
        "ALPHA = 30\n",
        "COS_N = 5\n",
        "COS_L = 2\n",
        "GROUPS_NUM = 1\n",
        "MIN_IMAGES_PER_CLASS = 10\n",
        "# Model parameters\n",
        "BACKBONE = \"resnet18\"   # [\"vgg16\", \"resnet18\", \"resnet50\", \"resnet101\", \"resnet152\"]\n",
        "FC_OUTPUT_DIM = 512     # Output dimension of final fully connected layer\n",
        "# Training parameters\n",
        "USE_AMP_16 = \"store_true\" # use Automatic Mixed Precision\n",
        "AUGMENTATION_DEVICE = \"cuda\" # [\"cuda\", \"cpu\"]\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_NUM = 3\n",
        "ITERATIONS_PER_EPOCH = 10_000\n",
        "LR = 0.00001            # Learning rate  \n",
        "CLASSIFIERS_LR = 0.01\n",
        "# Data augmentation\n",
        "BRIGHTNESS = 0.7\n",
        "CONTRAST = 0.7\n",
        "SATURATION = 0.7\n",
        "HUE = 0.5\n",
        "RANDOM_RESIZED_CROP = 0.5\n",
        "# Validation / test parameters\n",
        "INFER_BATCH_SIZE = 16 # Batch size for inference (validating and testing)\n",
        "POSITIVE_DIST_THRESHOLD = 25 # distance in meters for a prediction to be considered a positive\n",
        "# Resume parameters\n",
        "RESUME_TRAIN = None # path to checkpoint to resume, e.g. logs/.../last_checkpoint.pth\n",
        "RESUME_MODEL = None     # Path to model to resume training from\n",
        "# Other parameters\n",
        "DEVICE = \"cuda\" # [\"cuda\", \"cpu\"]\n",
        "SEED = 0\n",
        "NUM_WORKERS = 8\n",
        "DATASET_FOLDER = \"/content/small\" # path of the folder with train/val sets\n",
        "SAVEDIR = \"default\"\n",
        "\n",
        "\n",
        "if not os.path.exists(DATASET_FOLDER):\n",
        "    raise FileNotFoundError(f\"Dataset folder {DATASET_FOLDER} not found\")\n",
        "\n",
        "train_set_folder = os.path.join(DATASET_FOLDER, \"train\")\n",
        "\n",
        "if not os.path.exists(train_set_folder):\n",
        "    raise FileNotFoundError(f\"Train set folder {train_set_folder} not found\")\n",
        "\n",
        "val_set_folder = os.path.join(DATASET_FOLDER, \"val\")\n",
        "\n",
        "if not os.path.exists(val_set_folder):\n",
        "    raise FileNotFoundError(f\"Validation set folder {val_set_folder} not found\")\n",
        "\n",
        "\n",
        "# dictionary for the parameters\n",
        "args = {\n",
        "    'M': COS_M, 'alpha': ALPHA, 'N': COS_N, 'L': COS_L, 'groups_num': GROUPS_NUM,\n",
        "    'min_images_per_class': MIN_IMAGES_PER_CLASS, 'backbone': BACKBONE,\n",
        "    'fc_output_dim': FC_OUTPUT_DIM, 'use_amp_16': USE_AMP_16,\n",
        "    'augmentation_device': AUGMENTATION_DEVICE, 'batch_size': BATCH_SIZE,\n",
        "    'epochs_num': EPOCHS_NUM, 'iterations_per_epoch': ITERATIONS_PER_EPOCH,\n",
        "    'lr': LR, 'classifiers_lr': CLASSIFIERS_LR, 'brightness': BRIGHTNESS,\n",
        "    'contrast': CONTRAST, 'hue': HUE, 'saturation': SATURATION,\n",
        "    'random_resized_crop': RANDOM_RESIZED_CROP, 'infer_batch_size': INFER_BATCH_SIZE,\n",
        "    'positive_dist_threshold': POSITIVE_DIST_THRESHOLD, 'resume_train': RESUME_TRAIN,\n",
        "    'resume_model': RESUME_MODEL, 'device': DEVICE, 'seed': SEED,\n",
        "    'num_workers': NUM_WORKERS, 'dataset_folder': DATASET_FOLDER, 'savedir': SAVEDIR\n",
        "}\n",
        "\n",
        "args = dotdict(args)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import sys\n",
        "import torch\n",
        "import logging\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import multiprocessing\n",
        "from datetime import datetime\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import test\n",
        "import util\n",
        "import parser\n",
        "import commons\n",
        "import cosface_loss\n",
        "import augmentations\n",
        "from model import network\n",
        "from datasets.test_dataset import TestDataset\n",
        "from datasets.train_dataset import TrainDataset\n",
        "\n",
        "torch.backends.cudnn.benchmark = True  # Provides a speedup\n",
        "\n",
        "args = parser.parse_arguments()\n",
        "start_time = datetime.now()\n",
        "output_folder = f\"logs/{args.save_dir}/{start_time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
        "commons.make_deterministic(args.seed)\n",
        "commons.setup_logging(output_folder, console=\"debug\")\n",
        "logging.info(\" \".join(sys.argv))\n",
        "logging.info(f\"Arguments: {args}\")\n",
        "logging.info(f\"The outputs are being saved in {output_folder}\")\n",
        "\n",
        "#### Model\n",
        "model = network.GeoLocalizationNet(args.backbone, args.fc_output_dim)\n",
        "\n",
        "logging.info(f\"There are {torch.cuda.device_count()} GPUs and {multiprocessing.cpu_count()} CPUs.\")\n",
        "\n",
        "if args.resume_model is not None:\n",
        "    logging.debug(f\"Loading model from {args.resume_model}\")\n",
        "    model_state_dict = torch.load(args.resume_model)\n",
        "    model.load_state_dict(model_state_dict)\n",
        "\n",
        "model = model.to(args.device).train()\n",
        "\n",
        "#### Optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "model_optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "#### Datasets\n",
        "groups = [TrainDataset(args, args.train_set_folder, M=args.M, alpha=args.alpha, N=args.N, L=args.L,\n",
        "                       current_group=n, min_images_per_class=args.min_images_per_class) for n in range(args.groups_num)]\n",
        "# Each group has its own classifier, which depends on the number of classes in the group\n",
        "classifiers = [cosface_loss.MarginCosineProduct(args.fc_output_dim, len(group)) for group in groups]\n",
        "classifiers_optimizers = [torch.optim.Adam(classifier.parameters(), lr=args.classifiers_lr) for classifier in classifiers]\n",
        "\n",
        "logging.info(f\"Using {len(groups)} groups\")\n",
        "logging.info(f\"The {len(groups)} groups have respectively the following number of classes {[len(g) for g in groups]}\")\n",
        "logging.info(f\"The {len(groups)} groups have respectively the following number of images {[g.get_images_num() for g in groups]}\")\n",
        "\n",
        "val_ds = TestDataset(args.val_set_folder, positive_dist_threshold=args.positive_dist_threshold)\n",
        "logging.info(f\"Validation set: {val_ds}\")\n",
        "\n",
        "#### Resume\n",
        "if args.resume_train:\n",
        "    model, model_optimizer, classifiers, classifiers_optimizers, best_val_recall1, start_epoch_num = \\\n",
        "        util.resume_train(args, output_folder, model, model_optimizer, classifiers, classifiers_optimizers)\n",
        "    model = model.to(args.device)\n",
        "    epoch_num = start_epoch_num - 1\n",
        "    logging.info(f\"Resuming from epoch {start_epoch_num} with best R@1 {best_val_recall1:.1f} from checkpoint {args.resume_train}\")\n",
        "else:\n",
        "    best_val_recall1 = start_epoch_num = 0\n",
        "\n",
        "#### Train / evaluation loop\n",
        "logging.info(\"Start training ...\")\n",
        "logging.info(f\"There are {len(groups[0])} classes for the first group, \" +\n",
        "             f\"each epoch has {args.iterations_per_epoch} iterations \" +\n",
        "             f\"with batch_size {args.batch_size}, therefore the model sees each class (on average) \" +\n",
        "             f\"{args.iterations_per_epoch * args.batch_size / len(groups[0]):.1f} times per epoch\")\n",
        "\n",
        "\n",
        "if args.augmentation_device == \"cuda\":\n",
        "    gpu_augmentation = T.Compose([\n",
        "            augmentations.DeviceAgnosticColorJitter(brightness=args.brightness,\n",
        "                                                    contrast=args.contrast,\n",
        "                                                    saturation=args.saturation,\n",
        "                                                    hue=args.hue),\n",
        "            augmentations.DeviceAgnosticRandomResizedCrop([512, 512],\n",
        "                                                          scale=[1-args.random_resized_crop, 1]),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "if args.use_amp16:\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "for epoch_num in range(start_epoch_num, args.epochs_num):\n",
        "    \n",
        "    #### Train\n",
        "    epoch_start_time = datetime.now()\n",
        "    # Select classifier and dataloader according to epoch\n",
        "    current_group_num = epoch_num % args.groups_num\n",
        "    classifiers[current_group_num] = classifiers[current_group_num].to(args.device)\n",
        "    util.move_to_device(classifiers_optimizers[current_group_num], args.device)\n",
        "    \n",
        "    dataloader = commons.InfiniteDataLoader(groups[current_group_num], num_workers=args.num_workers,\n",
        "                                            batch_size=args.batch_size, shuffle=True,\n",
        "                                            pin_memory=(args.device == \"cuda\"), drop_last=True)\n",
        "    \n",
        "    dataloader_iterator = iter(dataloader)\n",
        "    model = model.train()\n",
        "    \n",
        "    epoch_losses = np.zeros((0, 1), dtype=np.float32)\n",
        "    for iteration in tqdm(range(args.iterations_per_epoch), ncols=100):\n",
        "        images, targets, _ = next(dataloader_iterator)\n",
        "        images, targets = images.to(args.device), targets.to(args.device)\n",
        "        \n",
        "        if args.augmentation_device == \"cuda\":\n",
        "            images = gpu_augmentation(images)\n",
        "        \n",
        "        model_optimizer.zero_grad()\n",
        "        classifiers_optimizers[current_group_num].zero_grad()\n",
        "        \n",
        "        if not args.use_amp16:\n",
        "            descriptors = model(images)\n",
        "            output = classifiers[current_group_num](descriptors, targets)\n",
        "            loss = criterion(output, targets)\n",
        "            loss.backward()\n",
        "            epoch_losses = np.append(epoch_losses, loss.item())\n",
        "            del loss, output, images\n",
        "            model_optimizer.step()\n",
        "            classifiers_optimizers[current_group_num].step()\n",
        "        else:  # Use AMP 16\n",
        "            with torch.cuda.amp.autocast():\n",
        "                descriptors = model(images)\n",
        "                output = classifiers[current_group_num](descriptors, targets)\n",
        "                loss = criterion(output, targets)\n",
        "            scaler.scale(loss).backward()\n",
        "            epoch_losses = np.append(epoch_losses, loss.item())\n",
        "            del loss, output, images\n",
        "            scaler.step(model_optimizer)\n",
        "            scaler.step(classifiers_optimizers[current_group_num])\n",
        "            scaler.update()\n",
        "    \n",
        "    classifiers[current_group_num] = classifiers[current_group_num].cpu()\n",
        "    util.move_to_device(classifiers_optimizers[current_group_num], \"cpu\")\n",
        "    \n",
        "    logging.debug(f\"Epoch {epoch_num:02d} in {str(datetime.now() - epoch_start_time)[:-7]}, \"\n",
        "                  f\"loss = {epoch_losses.mean():.4f}\")\n",
        "    \n",
        "    #### Evaluation\n",
        "    recalls, recalls_str = test.test(args, val_ds, model)\n",
        "    logging.info(f\"Epoch {epoch_num:02d} in {str(datetime.now() - epoch_start_time)[:-7]}, {val_ds}: {recalls_str[:20]}\")\n",
        "    is_best = recalls[0] > best_val_recall1\n",
        "    best_val_recall1 = max(recalls[0], best_val_recall1)\n",
        "    # Save checkpoint, which contains all training parameters\n",
        "    util.save_checkpoint({\n",
        "        \"epoch_num\": epoch_num + 1,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": model_optimizer.state_dict(),\n",
        "        \"classifiers_state_dict\": [c.state_dict() for c in classifiers],\n",
        "        \"optimizers_state_dict\": [c.state_dict() for c in classifiers_optimizers],\n",
        "        \"best_val_recall1\": best_val_recall1\n",
        "    }, is_best, output_folder)\n",
        "\n",
        "\n",
        "logging.info(f\"Trained for {epoch_num+1:02d} epochs, in total in {str(datetime.now() - start_time)[:-7]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPIfX1yp_dtj"
      },
      "source": [
        "Save all logs generated by training in a specific folder in personal gdrive. Remember to copy inside shared_data of project drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnPzKaCQ-xRD",
        "outputId": "ff570806-f650-4bb4-dcef-7989007d3d87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "  adding: content/logs/ (stored 0%)\n",
            "  adding: content/logs/content/ (stored 0%)\n",
            "  adding: content/logs/content/saved_models/ (stored 0%)\n",
            "  adding: content/logs/content/saved_models/2022-12-11_17-17-27/ (stored 0%)\n",
            "  adding: content/logs/content/saved_models/2022-12-11_17-17-27/info.log (deflated 58%)\n",
            "  adding: content/logs/content/saved_models/2022-12-11_17-17-27/debug.log (deflated 66%)\n",
            "  adding: content/logs/content/saved_models/2022-12-11_17-17-27/last_checkpoint.pth (deflated 8%)\n",
            "  adding: content/logs/content/saved_models/2022-12-11_17-17-27/best_model.pth (deflated 7%)\n",
            "  adding: content/cache/ (stored 0%)\n",
            "  adding: content/cache/small_M10_N5_mipc10.torch (deflated 74%)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# zip logs -> logs.zip\n",
        "!zip -r /content/drive/MyDrive/logs.zip /content/logs/\n",
        "# zip cache -> cache.zip\n",
        "!zip -r /content/drive/MyDrive/cache.zip /content/cache/\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5jJf7v5Ox91"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import logging\n",
        "import multiprocessing\n",
        "from datetime import datetime\n",
        "\n",
        "import test\n",
        "import parser\n",
        "import commons\n",
        "from model import network\n",
        "from datasets.test_dataset import TestDataset\n",
        "\n",
        "torch.backends.cudnn.benchmark = True  # Provides a speedup\n",
        "\n",
        "args = parser.parse_arguments(is_training=False)\n",
        "start_time = datetime.now()\n",
        "output_folder = f\"logs/{args.save_dir}/{start_time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
        "commons.make_deterministic(args.seed)\n",
        "commons.setup_logging(output_folder, console=\"info\")\n",
        "logging.info(\" \".join(sys.argv))\n",
        "logging.info(f\"Arguments: {args}\")\n",
        "logging.info(f\"The outputs are being saved in {output_folder}\")\n",
        "\n",
        "#### Model\n",
        "model = network.GeoLocalizationNet(args.backbone, args.fc_output_dim)\n",
        "\n",
        "logging.info(f\"There are {torch.cuda.device_count()} GPUs and {multiprocessing.cpu_count()} CPUs.\")\n",
        "\n",
        "if args.resume_model is not None:\n",
        "    logging.info(f\"Loading model from {args.resume_model}\")\n",
        "    model_state_dict = torch.load(args.resume_model)\n",
        "    model.load_state_dict(model_state_dict)\n",
        "else:\n",
        "    logging.info(\"WARNING: You didn't provide a path to resume the model (--resume_model parameter). \" +\n",
        "                 \"Evaluation will be computed using randomly initialized weights.\")\n",
        "\n",
        "model = model.to(args.device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test on SF-XS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XZjk3Z2O1m6",
        "outputId": "7b469c52-5816-4523-dfba-e56e5e54a3f0"
      },
      "outputs": [],
      "source": [
        "# dataset_folder is the same of the training\n",
        "test_set_folder = os.path.join(DATASET_FOLDER, \"test\")\n",
        "\n",
        "if not os.path.exists(test_set_folder):\n",
        "    raise FileNotFoundError(f\"Test set folder {test_set_folder} not found\")\n",
        "\n",
        "test_ds = TestDataset(args.test_set_folder, queries_folder=\"queries_v1\",\n",
        "                      positive_dist_threshold=args.positive_dist_threshold)\n",
        "\n",
        "recalls, recalls_str = test.test(args, test_ds, model)\n",
        "logging.info(f\"{test_ds}: {recalls_str}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJgmqoCgPxwu",
        "outputId": "bf17d14d-3d91-4555-b6fa-39bd3d3ffc9e"
      },
      "outputs": [],
      "source": [
        "# TEST ON TOKYO-XS\n",
        "tokyo_xs_folder = \"/content/tokyo_xs\"\n",
        "test_set_folder = os.path.join(tokyo_xs_folder, \"test\")\n",
        "\n",
        "if not os.path.exists(test_set_folder):\n",
        "    raise FileNotFoundError(f\"Test set folder {test_set_folder} not found\")\n",
        "\n",
        "test_ds = TestDataset(args.test_set_folder,\n",
        "                      positive_dist_threshold=args.positive_dist_threshold)\n",
        "\n",
        "recalls, recalls_str = test.test(args, test_ds, model)\n",
        "logging.info(f\"{test_ds}: {recalls_str}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9Vzh7YNQrQa"
      },
      "outputs": [],
      "source": [
        "# TEST ON TOKYO-NIGHT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wYgYrIrRUis"
      },
      "outputs": [],
      "source": [
        "# SAVE ON DRIVE\n",
        "!zip -r /content/drive/MyDrive/cache.zip /content/cache/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM5blY4zJYoIgO1o0c+0TIz",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8 (default, Apr 13 2021, 12:59:45) \n[Clang 10.0.0 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
