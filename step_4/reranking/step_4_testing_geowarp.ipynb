{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIQIs6cYYyak"
      },
      "source": [
        "The purpose of this notebook is to load the default cosplace model trained on sf_xs and evaluate recalls of various already trained re ranking models provided by [GeoWarp](https://github.com/gmberton/geo_warp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf3z5SqWZ91b"
      },
      "source": [
        "# pip install requirements\n",
        "\n",
        "Remember to click on \"Restart Runtime\" before go on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YbtEmI1AiTkF",
        "outputId": "e1d84b35-f961-4e1c-fd60-ddbcfac02d49"
      },
      "outputs": [],
      "source": [
        "# CosPlace requirements\n",
        "!pip3 install \"faiss_cpu>=1.7.1\"\n",
        "!pip3 install \"numpy>=1.21.2\"\n",
        "!pip3 install \"Pillow>=9.0.1\"\n",
        "!pip3 install \"scikit_learn>=1.0.2\"\n",
        "!pip3 install \"torch>=1.8.2\"\n",
        "!pip3 install \"torchvision>=0.9.2\"\n",
        "!pip3 install \"tqdm>=4.62.3\"\n",
        "!pip3 install \"utm>=0.7.0\"\n",
        "!pip3 install \"kornia==0.6.2\"\n",
        "!pip3 install \"timm\"\n",
        "\n",
        "import torch\n",
        "#use GPU if available \n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #'cpu' # 'cuda' or 'cpu'\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czjvnq3FjBmh"
      },
      "source": [
        "# Download Datasets and previous data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hY8bYPGyqLF"
      },
      "source": [
        "Downloading with gdown doesn't work properly.\n",
        "\n",
        "Prefer always to use drive / mount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGOhXMNqjMed",
        "outputId": "2535b70c-a73e-4c29-a5a2-a629981dec14"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gdown\n",
        "from google.colab import drive\n",
        "\n",
        "def download(id, output=None, quiet=True):\n",
        "  gdown.download(\n",
        "    f\"https://drive.google.com/uc?export=download&confirm=pbef&id={id}\",\n",
        "    output=output,\n",
        "    quiet=quiet\n",
        "  )\n",
        "\n",
        "\n",
        "use_mount = True\n",
        "if use_mount:\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "# TOKYO-XS DATASET\n",
        "if not os.path.isdir(\"/content/tokyo_xs\"):\n",
        "  if use_mount:\n",
        "    !jar xvf \"/content/drive/MyDrive/Project 6 - Dataset/tokyo-xs.zip\"\n",
        "  else:\n",
        "    id = \"1fBCnap5BRh36474cVkjvjlC-yUTEb1n3\"\n",
        "    download(id, quiet=False)                           # download from our gdrive\n",
        "    !jar xvf \"/content/tokyo-xs.zip\"                    # unzip\n",
        "    !rm -r \"/content/tokyo-xs.zip\"                      # remove .zip file\n",
        "\n",
        "if not os.path.isdir(\"/content/tokyo_xs\"):\n",
        "  raise FileNotFoundError(f\"Can't download tokyo xs\")\n",
        "\n",
        "#TOKYO NIGHT DATASET\n",
        "if not os.path.isdir(\"/content/tokyo-night\"):\n",
        "  if use_mount:\n",
        "    !jar xvf \"/content/drive/MyDrive/Project 6 - Dataset/tokyo-night.zip\"\n",
        "  else:\n",
        "    id = \"1EZJY2r5565-iVk2oEbTns0xc4F_em4Y4\"\n",
        "    download(id, quiet=False)                           # download from our gdrive\n",
        "    !jar xvf \"/content/tokyo-night.zip\"                    # unzip\n",
        "    !rm -r \"/content/tokyo-night.zip\"\n",
        "\n",
        "if not os.path.isdir(\"/content/tokyo-night\"):\n",
        "  raise FileNotFoundError(f\"Can't download tokyo night\")\n",
        "\n",
        "# SAN FRANCISCO - XS DATASET\n",
        "if not os.path.isdir(\"/content/small\"):\n",
        "  if use_mount:\n",
        "    !jar xvf \"/content/drive/MyDrive/Project 6 - Dataset/sf-xs.zip\"\n",
        "  else:\n",
        "    id = \"1brIxBJmOgvuzFbI57f5LxnMxjccUu993\"\n",
        "    download(id, quiet=False)                           # download\n",
        "    !jar xvf \"/content/sf-xs.zip\"                       # unzip\n",
        "    !rm -r \"/content/sf-xs.zip\"                         # remove .zip file\n",
        "\n",
        "if not os.path.isdir(\"/content/small\"):\n",
        "  raise FileNotFoundError(f\"Can't download sfxs\")\n",
        "\n",
        "# CHECK YOUR DRIVE FOLDER!\n",
        "# pretrained baselines for geowarp\n",
        "if not os.path.isdir(\"/content/pretrained_baselines/\"):\n",
        "  if use_mount:\n",
        "    !jar xvf \"/content/drive/MyDrive/project6/geowarp/pretrained_baselines.zip\"\n",
        "\n",
        "if not os.path.isdir(\"/content/pretrained_baselines/\"):\n",
        "  raise FileNotFoundError(f\"Can't download pretrained_baselines for geowarp\")\n",
        "\n",
        "# self trained baselines for geowarp\n",
        "if not os.path.isdir(\"/content/selftrained_baselines/\"):\n",
        "  if use_mount:\n",
        "    !jar xvf \"/content/drive/MyDrive/project6/geowarp/selftrained_baselines.zip\"\n",
        "\n",
        "if not os.path.isdir(\"/content/selftrained_baselines/\"):\n",
        "  raise FileNotFoundError(f\"Can't download selftrained_baselines for geowarp\")\n",
        "\n",
        "# CHECK YOUR DRIVE FOLDER!\n",
        "# trained hr for geowarp\n",
        "if not os.path.isdir(\"/content/trained_homography_regressions/\"):\n",
        "  if use_mount:\n",
        "    !jar xvf \"/content/drive/MyDrive/project6/geowarp/trained_homography_regressions.zip\"\n",
        "\n",
        "if not os.path.isdir(\"/content/trained_homography_regressions/\"):\n",
        "  raise FileNotFoundError(f\"Can't download trained_homography_regressions for geowarp\")\n",
        "\n",
        "# CHECK YOUR DRIVE FOLDER!\n",
        "#best model of CosPlace training\n",
        "if not os.path.isfile(\"/content/saved_models/default.pth\"):\n",
        "  if use_mount:\n",
        "    !mkdir \"/content/saved_models\"\n",
        "    !cp \"/content/drive/MyDrive/project6/geowarp/default.pth\" \"/content/saved_models/default.pth\"\n",
        "\n",
        "if not os.path.isfile(\"/content/saved_models/default.pth\"):\n",
        "  raise FileNotFoundError(f\"Can't find model to resume, please provide one\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g6SkCgyhl-g"
      },
      "source": [
        "# Download Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFrIIf6E0UQM"
      },
      "source": [
        "Clone of original repo of CosPlace and our code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63SgJ_Y0hwrC",
        "outputId": "0eecc2bc-0184-4683-bda8-c630bc742a69"
      },
      "outputs": [],
      "source": [
        "# download code of CosPlace\n",
        "if not os.path.isdir(\"/content/CosPlace\"):\n",
        "    !git clone \"https://github.com/gmberton/CosPlace\" \n",
        "\n",
        "# download our code\n",
        "if not os.path.isdir(\"/content/Team\"):\n",
        "    !git clone --single-branch --branch \"develop\" \"https://github.com/gab-palmeri/aml-geolocalization.git\"\n",
        "    !mv \"/content/aml-geolocalization/\" \"/content/Team\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiwWzjzSio-h"
      },
      "source": [
        "\n",
        "\n",
        "# Import Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAd54tr_cNtO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import logging\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import torchvision.transforms as T\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "sys.path.append(\"/content/CosPlace/\")\n",
        "sys.path.append(\"/content/geo_warp/\")\n",
        "sys.path.append(\"/content/Team/\")\n",
        "import CosPlace\n",
        "from CosPlace import *\n",
        "\n",
        "from Team import *\n",
        "\n",
        "torch.backends.cudnn.benchmark = True  # Provides a speedup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MsFj6mmaYJs"
      },
      "source": [
        "This class let us to access to dictionary keys like `dict.key` instead of `dict[\"key\"]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKoLeQ_1xGAA"
      },
      "outputs": [],
      "source": [
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpbRF7uZxGAB"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tOcIQ1Fwni7"
      },
      "source": [
        "## Setup parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI0BfY2_xGAB"
      },
      "outputs": [],
      "source": [
        "# CosPlace Groups parameters\n",
        "COS_M = 10\n",
        "ALPHA = 30\n",
        "COS_N = 5\n",
        "COS_L = 2\n",
        "GROUPS_NUM = 1\n",
        "MIN_IMAGES_PER_CLASS = 10\n",
        "# Model parameters\n",
        "BACKBONE = \"resnet18\"   # [\"resnet18\", \"efficientnet_v2_s\", \"mobilenet_v3_large\"]\n",
        "PRETRAIN = \"imagenet\"   # [\"imagenet\", \"places\", \"gldv2\"]\n",
        "FC_OUTPUT_DIM = 512     # Output dimension of final fully connected layer\n",
        "# Training parameters\n",
        "AUGMENTATION_DEVICE = \"cuda\"    # [\"cuda\", \"cpu\"]\n",
        "USE_AMP_16 = AUGMENTATION_DEVICE == \"cuda\"      # use Automatic Mixed Precision\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_NUM = 3\n",
        "ITERATIONS_PER_EPOCH = 10_000\n",
        "LR = 0.00001                    # Learning rate  \n",
        "CLASSIFIERS_LR = 0.01\n",
        "# Data augmentation\n",
        "BRIGHTNESS = 0.7\n",
        "CONTRAST = 0.7\n",
        "SATURATION = 0.7\n",
        "HUE = 0.5\n",
        "RANDOM_RESIZED_CROP = 0.5\n",
        "# Validation / test parameters\n",
        "INFER_BATCH_SIZE = 16           # Batch size for inference (validating and testing)\n",
        "POSITIVE_DIST_THRESHOLD = 25    # distance in meters for a prediction to be considered a positive\n",
        "# Resume parameters\n",
        "RESUME_TRAIN = None     # path to checkpoint to resume, e.g. logs/.../last_checkpoint.pth\n",
        "RESUME_MODEL = \"/content/saved_models/default.pth\"     # Path to model to resume for test\n",
        "# Other parameters\n",
        "DEVICE = \"cuda\"                     # [\"cuda\", \"cpu\"]\n",
        "SEED = 0\n",
        "NUM_WORKERS = 8\n",
        "DATASET_FOLDER = \"/content/small\"   # path of the folder with train/val sets\n",
        "\n",
        "\n",
        "if not os.path.exists(DATASET_FOLDER):\n",
        "    raise FileNotFoundError(f\"Dataset folder {DATASET_FOLDER} not found\")\n",
        "\n",
        "train_set_folder = os.path.join(DATASET_FOLDER, \"train\")\n",
        "\n",
        "if not os.path.exists(train_set_folder):\n",
        "    raise FileNotFoundError(f\"Train set folder {train_set_folder} not found\")\n",
        "\n",
        "val_set_folder = os.path.join(DATASET_FOLDER, \"val\")\n",
        "\n",
        "if not os.path.exists(val_set_folder):\n",
        "    raise FileNotFoundError(f\"Validation set folder {val_set_folder} not found\")\n",
        "\n",
        "\n",
        "# dictionary for the parameters\n",
        "args = {\n",
        "    'M': COS_M, 'alpha': ALPHA, 'N': COS_N, 'L': COS_L, 'groups_num': GROUPS_NUM,\n",
        "    'min_images_per_class': MIN_IMAGES_PER_CLASS, 'backbone': BACKBONE, \"pretrain\": PRETRAIN,\n",
        "    'fc_output_dim': FC_OUTPUT_DIM, 'use_amp16': USE_AMP_16,\n",
        "    'augmentation_device': AUGMENTATION_DEVICE, 'batch_size': BATCH_SIZE,\n",
        "    'epochs_num': EPOCHS_NUM, 'iterations_per_epoch': ITERATIONS_PER_EPOCH,\n",
        "    'lr': LR, 'classifiers_lr': CLASSIFIERS_LR, 'brightness': BRIGHTNESS,\n",
        "    'contrast': CONTRAST, 'hue': HUE, 'saturation': SATURATION,\n",
        "    'random_resized_crop': RANDOM_RESIZED_CROP, 'infer_batch_size': INFER_BATCH_SIZE,\n",
        "    'positive_dist_threshold': POSITIVE_DIST_THRESHOLD, 'resume_train': RESUME_TRAIN,\n",
        "    'resume_model': RESUME_MODEL, 'device': DEVICE, 'seed': SEED,\n",
        "    'num_workers': NUM_WORKERS, 'dataset_folder': DATASET_FOLDER,\n",
        "    'val_set_folder': val_set_folder, 'train_set_folder': train_set_folder,\n",
        "}\n",
        "\n",
        "# this helps to reuse the code from the original CosPlace\n",
        "args = dotdict(args)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weAbHrt3PsA3"
      },
      "source": [
        "# Setup logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF4OSAc0PsA3",
        "outputId": "5f91b332-eba2-4d20-fa30-660bb1055146"
      },
      "outputs": [],
      "source": [
        "from CosPlace import commons\n",
        "\n",
        "start_time = datetime.now()\n",
        "output_folder = f\"logs/{args.save_dir}/{start_time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
        "commons.make_deterministic(args.seed)\n",
        "commons.setup_logging(output_folder, console=None)\n",
        "logging.info(\" \".join(sys.argv))\n",
        "logging.info(f\"Arguments: {args}\")\n",
        "logging.info(f\"The outputs are being saved in {output_folder}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5jJf7v5Ox91"
      },
      "source": [
        "# Testing GeoWarp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ItObSaBl6ox"
      },
      "source": [
        "## Import model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzJGjw9xxGAE",
        "outputId": "a3231964-f890-446d-a718-21421c29e891"
      },
      "outputs": [],
      "source": [
        "from CosPlace import model, test, datasets\n",
        "from Team.model import network\n",
        "from datasets.test_dataset import TestDataset\n",
        "\n",
        "#### Model\n",
        "model = network.GeoLocalizationNet(args.backbone, args.fc_output_dim, args.pretrain)\n",
        "\n",
        "logging.info(f\"There are {torch.cuda.device_count()} GPUs and {multiprocessing.cpu_count()} CPUs.\")\n",
        "\n",
        "if args.resume_model is not None:\n",
        "  if os.path.exists(args.resume_model):\n",
        "    resume_model = args.resume_model\n",
        "\n",
        "if resume_model is not None:\n",
        "  if not os.path.exists(resume_model):\n",
        "    raise FileNotFoundError(f\"Model {resume_model} not found.\")\n",
        "\n",
        "  logging.info(f\"Loading model from {resume_model}\")\n",
        "  model_state_dict = torch.load(resume_model)\n",
        "  model.load_state_dict(model_state_dict)\n",
        "else:\n",
        "    logging.info(\"WARNING: You didn't provide a path to resume the model (--resume_model parameter). \" +\n",
        "                 \"Evaluation will be computed using randomly initialized weights.\")\n",
        "\n",
        "model = model.to(args.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDk5GqxCcuqk"
      },
      "source": [
        "## GeoWarp Settings"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Choose if you want to use the pretrained homography regression or the self trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selftrained = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selftrained = False"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDmu_M0BHEx4"
      },
      "outputs": [],
      "source": [
        "import itertools \n",
        "# architectures for which we have pretrained feature extractor\n",
        "archs = [\"alexnet\", \"resnet50\", \"vgg16\"]\n",
        "# pooling layer\n",
        "poolings = [\"gem\", \"netvlad\"]\n",
        "# num_reranked_predictions\n",
        "nums_reranked = [5, 10, 15]\n",
        "\n",
        "combinations = itertools.product(archs, poolings, nums_reranked)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kDZGohMxgG1x"
      },
      "source": [
        "## Function to calculate predictions and recalls for GeoLocalizationNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJXz7DT1gK8Q"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import torch\n",
        "import logging\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from typing import Tuple\n",
        "from argparse import Namespace\n",
        "from torch.utils.data.dataset import Subset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import Team.GeoWarp.network as network\n",
        "import Team.GeoWarp.util as util\n",
        "import Team.GeoWarp.dataset_warp as dataset_warp\n",
        "\n",
        "def calculate_preds_and_recalls(args: Namespace, eval_ds: Dataset, model: torch.nn.Module):\n",
        "    \"\"\"Compute descriptors of the given dataset and compute the recalls.\"\"\"\n",
        "    \n",
        "    model = model.eval()\n",
        "    with torch.no_grad():\n",
        "        logging.debug(\"Extracting database descriptors for evaluation/testing\")\n",
        "        database_subset_ds = Subset(eval_ds, list(range(eval_ds.database_num)))\n",
        "        database_dataloader = DataLoader(dataset=database_subset_ds, num_workers=args.num_workers,\n",
        "                                         batch_size=args.infer_batch_size, pin_memory=(args.device == \"cuda\"))\n",
        "        all_descriptors = np.empty((len(eval_ds), args.fc_output_dim), dtype=\"float32\")\n",
        "        for images, indices in tqdm(database_dataloader, ncols=100):\n",
        "            descriptors = model(images.to(args.device))\n",
        "            descriptors = descriptors.cpu().numpy()\n",
        "            all_descriptors[indices.numpy(), :] = descriptors\n",
        "        \n",
        "        logging.debug(\"Extracting queries descriptors for evaluation/testing using batch size 1\")\n",
        "        queries_infer_batch_size = 1\n",
        "        queries_subset_ds = Subset(eval_ds, list(range(eval_ds.database_num, eval_ds.database_num+eval_ds.queries_num)))\n",
        "        queries_dataloader = DataLoader(dataset=queries_subset_ds, num_workers=args.num_workers,\n",
        "                                        batch_size=queries_infer_batch_size, pin_memory=(args.device == \"cuda\"))\n",
        "        for images, indices in tqdm(queries_dataloader, ncols=100):\n",
        "            descriptors = model(images.to(args.device))\n",
        "            descriptors = descriptors.cpu().numpy()\n",
        "            all_descriptors[indices.numpy(), :] = descriptors\n",
        "    \n",
        "    queries_descriptors = all_descriptors[eval_ds.database_num:]\n",
        "    database_descriptors = all_descriptors[:eval_ds.database_num]\n",
        "    \n",
        "    # Use a kNN to find predictions\n",
        "    faiss_index = faiss.IndexFlatL2(args.fc_output_dim)\n",
        "    faiss_index.add(database_descriptors)\n",
        "    del database_descriptors, all_descriptors\n",
        "    \n",
        "    logging.debug(\"Calculating recalls\")\n",
        "    _, predictions = faiss_index.search(queries_descriptors, max(RECALL_VALUES))\n",
        "    \n",
        "    #### For each query, check if the predictions are correct\n",
        "    positives_per_query = eval_ds.get_positives()\n",
        "    recalls = np.zeros(len(RECALL_VALUES))\n",
        "    for query_index, preds in enumerate(predictions):\n",
        "        for i, n in enumerate(RECALL_VALUES):\n",
        "            if np.any(np.in1d(preds[:n], positives_per_query[query_index])):\n",
        "                recalls[i:] += 1\n",
        "                break\n",
        "    # Divide by queries_num and multiply by 100, so the recalls are in percentages\n",
        "    recalls = recalls / eval_ds.queries_num * 100\n",
        "    recalls_str = \", \".join([f\"R@{val}: {rec:.1f}\" for val, rec in zip(RECALL_VALUES, recalls)])\n",
        "    return predictions, recalls, recalls_str\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MIpsfVeGfVlr"
      },
      "source": [
        "## GeoWarp test function\n",
        "starting from predictions, arch, pooling and paths (all args) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hP5Oemv0Gfg9"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Compute R@1, R@5, R@10, R@20\n",
        "RECALL_VALUES = [1, 5, 10, 20]\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def open_image_and_apply_transform(image_path):\n",
        "    \"\"\"Given the path of an image, open the image, and return it as a normalized tensor.\n",
        "    \"\"\"\n",
        "    \n",
        "    pil_image = Image.open(image_path)\n",
        "    tensor_image = transform(pil_image)\n",
        "    return tensor_image\n",
        "\n",
        "\n",
        "def geo_warp_test(warp_model, predictions, test_dataset, num_reranked_predictions=5, recall_values=[1, 5, 10, 20], test_batch_size=8):\n",
        "    \n",
        "    warp_model.eval()\n",
        "    reranked_predictions = predictions.copy()\n",
        "\n",
        "    recalls = None\n",
        "    recalls_pretty_str = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for num_q in tqdm(range(test_dataset.queries_num), desc=\"Testing\", ncols=100):\n",
        "              dot_prods_wqp = np.zeros((num_reranked_predictions))\n",
        "              query_path = test_dataset.queries_paths[num_q]\n",
        "              for i1 in range(0, num_reranked_predictions, test_batch_size):\n",
        "                  batch_indexes = list(range(num_reranked_predictions))[i1:i1+test_batch_size]\n",
        "                  current_batch_size = len(batch_indexes)\n",
        "                  query = open_image_and_apply_transform(query_path)\n",
        "                  query_repeated_twice = torch.repeat_interleave(query.unsqueeze(0), current_batch_size, 0)\n",
        "                  \n",
        "                  preds = []\n",
        "                  for i in batch_indexes:\n",
        "                      pred_path = test_dataset.database_paths[predictions[num_q, i]]\n",
        "                      preds.append(open_image_and_apply_transform(pred_path))\n",
        "                  preds = torch.stack(preds)\n",
        "                  \n",
        "                  warped_pair = dataset_warp.compute_warping(warp_model, query_repeated_twice.cuda(), preds.cuda())\n",
        "                  q_features = warp_model(\"features_extractor\", [warped_pair[0], \"local\"])\n",
        "                  p_features = warp_model(\"features_extractor\", [warped_pair[1], \"local\"])\n",
        "                  # Sum along all axes except for B. wqp stands for warped query-prediction\n",
        "                  dot_prod_wqp = (q_features * p_features).sum(list(range(1, len(p_features.shape)))).cpu().detach().numpy()\n",
        "                  \n",
        "                  dot_prods_wqp[i1:i1+test_batch_size] = dot_prod_wqp\n",
        "              \n",
        "              reranking_indexes = dot_prods_wqp.argsort()[::-1]\n",
        "              reranked_predictions[num_q, :num_reranked_predictions] = predictions[num_q][reranking_indexes]\n",
        "      \n",
        "      ground_truths = test_dataset.get_positives()\n",
        "      recalls, recalls_pretty_str = util.compute_recalls(reranked_predictions, ground_truths, test_dataset, RECALL_VALUES)\n",
        "    return recalls, recalls_pretty_str\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVEznMFwxGAE"
      },
      "source": [
        "## Test on SF-XS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp7ynauVfjv7"
      },
      "source": [
        "### Calculate predictions for SF-XS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c3YrNp3f6qE",
        "outputId": "2b26d037-327a-47a2-fbd6-056a77aaedcf"
      },
      "outputs": [],
      "source": [
        "# dataset_folder is the same of the training\n",
        "test_set_folder = os.path.join(DATASET_FOLDER, \"test\")\n",
        "if not os.path.exists(test_set_folder):\n",
        "    raise FileNotFoundError(f\"Test set folder {test_set_folder} not found\")\n",
        "\n",
        "test_ds = TestDataset(test_set_folder, queries_folder=\"queries_v1\",\n",
        "                      positive_dist_threshold=args.positive_dist_threshold)\n",
        "\n",
        "sf_xs_preds, recalls, recalls_str = calculate_preds_and_recalls(args, test_ds, model)\n",
        "logging.info(f\"{test_ds}: {recalls_str}\")\n",
        "\n",
        "# recalls for csv file\n",
        "sf_xs_r15 = f\"{recalls[0]:.1f}/{recalls[1]:.1f}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3pY8Y_dqCKT"
      },
      "source": [
        "Benchmark geowarp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7IYBLaBqBX9",
        "outputId": "390361be-f663-44af-8e9c-da493062b77f"
      },
      "outputs": [],
      "source": [
        "combinations = itertools.product(archs, poolings, nums_reranked)\n",
        "sf_xs_rerank = []\n",
        "\n",
        "for arch, pooling, num_rerank in combinations:\n",
        "  if arch == \"vgg16\" and not num_rerank==5: continue\n",
        "  if selftrained and arch == \"resnet50\" and pooling == \"netvlad\": continue\n",
        "  ## MODEL\n",
        "  features_extractor = network.FeaturesExtractor(arch, pooling)\n",
        "  #global_features_dim = commons_warp.get_output_dim(features_extractor, \"GEM\")\n",
        "                \n",
        "  state_dict = torch.load(f\"/content/pretrained_baselines/{arch}_{pooling}.pth\")\n",
        "  features_extractor.load_state_dict(state_dict)\n",
        "  del state_dict\n",
        "                \n",
        "  state_dict = torch.load(f\"/content/trained_homography_regressions/{arch}_{pooling}.pth\" if not selftrained else f\"/content/selftrained_baselines/{arch}_{pooling}.pth\")\n",
        "  homography_regression = network.HomographyRegression(kernel_sizes=[7, 5, 5, 5, 5, 5], channels=[225, 128, 128, 64, 64, 64, 64], padding=1)\n",
        "  homography_regression.load_state_dict(state_dict)\n",
        "  del state_dict\n",
        "                \n",
        "  warp_model = network.Network(features_extractor, homography_regression).cuda().eval()\n",
        "  warp_model = torch.nn.DataParallel(warp_model)\n",
        "  \n",
        "  batch_size = 8\n",
        "  recalls, recalls_str = geo_warp_test(warp_model=warp_model, predictions=sf_xs_preds, test_dataset=test_ds, num_reranked_predictions=num_rerank, test_batch_size = batch_size)\n",
        "  if selftrained:\n",
        "    logging.info(\"self-trained models\")\n",
        "  logging.info(f\"arch: {arch}, pooling: {pooling}, num_rerank: {num_rerank}\")\n",
        "  logging.info(f\"{test_ds}: {recalls_str}\")\n",
        "  sf_xs_rerank.append(recalls)\n",
        "\n",
        "  del warp_model\n",
        "  del features_extractor\n",
        "  del homography_regression\n",
        "\n",
        "del sf_xs_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q28MxTcweIZ"
      },
      "source": [
        "## Test on Tokyo-XS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RH0JcMkmlQA"
      },
      "source": [
        "### Calculate predictions for Tokyo XS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7etnwT4XmlQB",
        "outputId": "07db5236-ea62-471a-dbcb-a747e98870ad"
      },
      "outputs": [],
      "source": [
        "tokyo_xs_folder = \"/content/tokyo_xs\"\n",
        "test_set_folder = os.path.join(tokyo_xs_folder, \"test\")\n",
        "if not os.path.exists(test_set_folder):\n",
        "    raise FileNotFoundError(f\"Test set folder {test_set_folder} not found\")\n",
        "\n",
        "test_ds = TestDataset(test_set_folder,\n",
        "                      positive_dist_threshold=args.positive_dist_threshold)\n",
        "\n",
        "tokyo_xs_preds, recalls, recalls_str = calculate_preds_and_recalls(args, test_ds, model)\n",
        "logging.info(f\"{test_ds}: {recalls_str}\")\n",
        "\n",
        "# recalls for csv file\n",
        "tokyo_xs_r15 = f\"{recalls[0]:.1f}/{recalls[1]:.1f}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoHZ7nzHmlQC"
      },
      "source": [
        "Benchmark geowarp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC70HTDumlQC",
        "outputId": "c83989ef-11f8-470b-fc8e-7614cbe45cca"
      },
      "outputs": [],
      "source": [
        "combinations = itertools.product(archs, poolings, nums_reranked)\n",
        "tokyo_xs_rerank = []\n",
        "for arch, pooling, num_rerank in combinations:\n",
        "  if arch == \"vgg16\" and not num_rerank==5: continue\n",
        "  if selftrained and arch == \"resnet50\" and pooling == \"netvlad\": continue\n",
        "  ## MODEL\n",
        "  features_extractor = network.FeaturesExtractor(arch, pooling)\n",
        "  #global_features_dim = commons_warp.get_output_dim(features_extractor, \"GEM\")\n",
        "                \n",
        "  state_dict = torch.load(f\"/content/pretrained_baselines/{arch}_{pooling}.pth\")\n",
        "  features_extractor.load_state_dict(state_dict)\n",
        "  del state_dict\n",
        "                \n",
        "  state_dict = torch.load(f\"/content/trained_homography_regressions/{arch}_{pooling}.pth\" if not selftrained else f\"/content/selftrained_baselines/{arch}_{pooling}.pth\")\n",
        "  homography_regression = network.HomographyRegression(kernel_sizes=[7, 5, 5, 5, 5, 5], channels=[225, 128, 128, 64, 64, 64, 64], padding=1)\n",
        "  homography_regression.load_state_dict(state_dict)\n",
        "  del state_dict\n",
        "                \n",
        "  warp_model = network.Network(features_extractor, homography_regression).cuda().eval()\n",
        "  warp_model = torch.nn.DataParallel(warp_model)\n",
        "\n",
        "  recalls, recalls_str = geo_warp_test(warp_model=warp_model, predictions=tokyo_xs_preds, test_dataset=test_ds, num_reranked_predictions=num_rerank)\n",
        "  if selftrained:\n",
        "    logging.info(\"self-trained models\")\n",
        "  logging.info(f\"arch: {arch}, pooling: {pooling}, num_rerank: {num_rerank}\")\n",
        "  logging.info(f\"{test_ds}: {recalls_str}\")\n",
        "  tokyo_xs_rerank.append(recalls)\n",
        "\n",
        "  del warp_model\n",
        "  del features_extractor\n",
        "  del homography_regression\n",
        "\n",
        "del tokyo_xs_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0tHKhzeweIZ"
      },
      "source": [
        "## Test on Tokyo-Night"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEJwYfVwnRo2"
      },
      "source": [
        "### Calculate predictions for Tokyo Night"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUaJekXTnRpC",
        "outputId": "4a2bbb6a-1927-486a-ac84-3935ceaa7cbe"
      },
      "outputs": [],
      "source": [
        "tokyo_night_folder = \"/content/tokyo-night/\"\n",
        "test_set_folder = os.path.join(tokyo_night_folder, \"test\")\n",
        "if not os.path.exists(test_set_folder):\n",
        "    raise FileNotFoundError(f\"Test set folder {test_set_folder} not found\")\n",
        "\n",
        "test_ds = TestDataset(test_set_folder,\n",
        "                      positive_dist_threshold=args.positive_dist_threshold)\n",
        "\n",
        "tokyo_night_preds, recalls, recalls_str = calculate_preds_and_recalls(args, test_ds, model)\n",
        "logging.info(f\"{test_ds}: {recalls_str}\")\n",
        "\n",
        "# recalls for csv file\n",
        "tokyo_night_r15 = f\"{recalls[0]:.1f}/{recalls[1]:.1f}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZPRABDPnRpC"
      },
      "source": [
        "Benchmark geowarp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GECzj8NgnRpD",
        "outputId": "ea72d68c-8607-49f6-d45e-0eb564eb5566"
      },
      "outputs": [],
      "source": [
        "combinations = itertools.product(archs, poolings, nums_reranked)\n",
        "tokyo_night_rerank = []\n",
        "for arch, pooling, num_rerank in combinations:\n",
        "  if arch == \"vgg16\" and not num_rerank==5: continue\n",
        "  if selftrained and arch == \"resnet50\" and pooling == \"netvlad\": continue\n",
        "  ## MODEL\n",
        "  features_extractor = network.FeaturesExtractor(arch, pooling)\n",
        "  #global_features_dim = commons_warp.get_output_dim(features_extractor, \"GEM\")\n",
        "                \n",
        "  state_dict = torch.load(f\"/content/pretrained_baselines/{arch}_{pooling}.pth\")\n",
        "  features_extractor.load_state_dict(state_dict)\n",
        "  del state_dict\n",
        "                \n",
        "  state_dict = torch.load(f\"/content/trained_homography_regressions/{arch}_{pooling}.pth\" if not selftrained else f\"/content/selftrained_baselines/{arch}_{pooling}.pth\")\n",
        "  homography_regression = network.HomographyRegression(kernel_sizes=[7, 5, 5, 5, 5, 5], channels=[225, 128, 128, 64, 64, 64, 64], padding=1)\n",
        "  homography_regression.load_state_dict(state_dict)\n",
        "  del state_dict\n",
        "                \n",
        "  warp_model = network.Network(features_extractor, homography_regression).cuda().eval()\n",
        "  warp_model = torch.nn.DataParallel(warp_model)\n",
        "\n",
        "  recalls, recalls_str = geo_warp_test(warp_model=warp_model, predictions=tokyo_night_preds, test_dataset=test_ds, num_reranked_predictions=num_rerank)\n",
        "  if selftrained:\n",
        "    logging.info(\"self-trained models\")\n",
        "  logging.info(f\"arch: {arch}, pooling: {pooling}, num_rerank: {num_rerank}\")\n",
        "  logging.info(f\"{test_ds}: {recalls_str}\")\n",
        "  tokyo_night_rerank.append(recalls)\n",
        "\n",
        "  del warp_model\n",
        "  del features_extractor\n",
        "  del homography_regression\n",
        "\n",
        "del tokyo_night_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVIvWwK2nnee"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbQvpzYEnpKi"
      },
      "source": [
        "## San Francisco XS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPmlz0zUnoxx"
      },
      "outputs": [],
      "source": [
        "logging.info(\"Original sfxs R1/R5\")\n",
        "logging.info(sf_xs_r15)\n",
        "\n",
        "combinations = itertools.product(archs, poolings, nums_reranked)\n",
        "for arch, pooling, num in combinations:\n",
        "  try:\n",
        "    logging.info(f\"Recalls sfxs for arch: {arch}, pooling: {pooling}, num_rerank: {num}\")\n",
        "  except:\n",
        "    break\n",
        "  logging.info(sf_xs_rerank)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W2ylSRRoQqL"
      },
      "source": [
        "## Tokyo XS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jqoxit0oWm5",
        "outputId": "7a780141-401c-429c-d148-3cdecd4256c9"
      },
      "outputs": [],
      "source": [
        "logging.info(\"Original tokyoxs R1/R5\")\n",
        "logging.info(tokyo_xs_r15)\n",
        "\n",
        "combinations = itertools.product(archs, poolings, nums_reranked)\n",
        "for arch, pooling, num in combinations:\n",
        "  try:\n",
        "    logging.info(f\"Recalls tokyoxs for arch: {arch}, pooling: {pooling}, num_rerank: {num}\")\n",
        "  except:\n",
        "    break\n",
        "  logging.info(tokyo_xs_rerank)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoIRrUN7ok7o"
      },
      "source": [
        "## Tokyo Night"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWzEGC_dooNc",
        "outputId": "3a791398-b3ad-4798-d08c-d7e54bab3c70"
      },
      "outputs": [],
      "source": [
        "logging.info(\"Original tokyo night R1/R5\")\n",
        "logging.info(tokyo_night_r15)\n",
        "\n",
        "combinations = itertools.product(archs, poolings, nums_reranked)\n",
        "for arch, pooling, num in combinations:\n",
        "  try:\n",
        "    logging.info(f\"Recalls tokyo night for arch: {arch}, pooling: {pooling}, num_rerank: {num}\")\n",
        "  except:\n",
        "    break\n",
        "  logging.info(tokyo_night_rerank)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27hqFtqPtWTS"
      },
      "source": [
        "# Save logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2fGDPm_tYUG",
        "outputId": "84a3f4d3-4f69-49f7-8808-59518c5d4baf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# zip logs -> logs.zip\n",
        "!zip -r /content/drive/MyDrive/project6/geowarp/logs.zip /content/logs/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "qf3z5SqWZ91b",
        "kDZGohMxgG1x",
        "MIpsfVeGfVlr"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
