{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gab-palmeri/aml-geolocalization/blob/sam/step_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "n18u6OFZxF_z"
      },
      "source": [
        "The purpose of this notebook is analyze the performance applying the following transformations to the images:\n",
        "- random horizontal flip\n",
        "- isotropic resize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf3z5SqWZ91b"
      },
      "source": [
        "# pip install requirements\n",
        "\n",
        "Remember to click on \"Restart Runtime\" before go on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YbtEmI1AiTkF",
        "outputId": "5d3b09bb-c84a-400a-e68f-e3a4ca7ab472"
      },
      "outputs": [],
      "source": [
        "# CosPlace requirements\n",
        "!pip3 install \"faiss_cpu>=1.7.1\"\n",
        "!pip3 install \"numpy>=1.21.2\"\n",
        "!pip3 install \"Pillow>=9.0.1\"\n",
        "!pip3 install \"scikit_learn>=1.0.2\"\n",
        "!pip3 install \"torch>=1.8.2\"\n",
        "!pip3 install \"torchvision>=0.9.2\"\n",
        "!pip3 install \"tqdm>=4.62.3\"\n",
        "!pip3 install \"utm>=0.7.0\"\n",
        "!pip3 install \"timm\"\n",
        "\n",
        "import torch\n",
        "#use GPU if available \n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #'cpu' # 'cuda' or 'cpu'\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czjvnq3FjBmh"
      },
      "source": [
        "# Download Datasets and previous data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hY8bYPGyqLF"
      },
      "source": [
        "Downloading with gdown doesn't work properly.\n",
        "\n",
        "Prefer always to use drive / mount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGOhXMNqjMed",
        "outputId": "4c11717c-1818-4386-d3bd-48bb13bc2a47"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gdown\n",
        "from google.colab import drive\n",
        "\n",
        "def download(id, output=None, quiet=True):\n",
        "  gdown.download(\n",
        "    f\"https://drive.google.com/uc?export=download&confirm=pbef&id={id}\",\n",
        "    output=output,\n",
        "    quiet=quiet\n",
        "  )\n",
        "\n",
        "\n",
        "use_mount = True\n",
        "resume_model = False\n",
        "\n",
        "if use_mount:\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "# TOKYO-XS DATASET\n",
        "if not os.path.isdir(\"/content/tokyo_xs\"):\n",
        "  if use_mount:\n",
        "    !jar xvf \"/content/drive/MyDrive/Project 6 - Dataset/tokyo-xs.zip\"\n",
        "  else:\n",
        "    id = \"1fBCnap5BRh36474cVkjvjlC-yUTEb1n3\"\n",
        "    download(id, quiet=False)                           # download from our gdrive\n",
        "    !jar xvf \"/content/tokyo-xs.zip\"                    # unzip\n",
        "    !rm -r \"/content/tokyo-xs.zip\"                      # remove .zip file\n",
        "\n",
        "if not os.path.isdir(\"/content/tokyo_xs\"):\n",
        "  raise FileNotFoundError(f\"Can't download tokyo xs\")\n",
        "\n",
        "#TOKYO NIGHT DATASET\n",
        "if not os.path.isdir(\"/content/tokyo-night\"):\n",
        "  if use_mount:\n",
        "    !jar xvf \"/content/drive/MyDrive/Project 6 - Dataset/tokyo-night.zip\"\n",
        "  else:\n",
        "    id = \"1tbEQL4XrUPaHyK5_OF16cpvam80PyttR\"\n",
        "    download(id, quiet=False)                           # download from our gdrive\n",
        "    !jar xvf \"/content/tokyo-night.zip\"                    # unzip\n",
        "    !rm -r \"/content/tokyo-night.zip\"\n",
        "\n",
        "if not os.path.isdir(\"/content/tokyo-night\"):\n",
        "  raise FileNotFoundError(f\"Can't download tokyo night\")\n",
        "\n",
        "# SAN FRANCISCO - XS DATASET\n",
        "if not os.path.isdir(\"/content/small\"):\n",
        "  if use_mount:\n",
        "    !jar xvf \"/content/drive/MyDrive/Project 6 - Dataset/sf-xs.zip\"\n",
        "  else:\n",
        "    id = \"1brIxBJmOgvuzFbI57f5LxnMxjccUu993\"\n",
        "    download(id, quiet=False)                           # download\n",
        "    !jar xvf \"/content/sf-xs.zip\"                       # unzip\n",
        "    !rm -r \"/content/sf-xs.zip\"                         # remove .zip file\n",
        "\n",
        "if not os.path.isdir(\"/content/small\"):\n",
        "  raise FileNotFoundError(f\"Can't download sfxs\")\n",
        "\n",
        "# resumed model\n",
        "if resume_model and not os.path.isfile(\"/content/saved_models/arcface_model.pth\"):\n",
        "  if use_mount:\n",
        "    !jar xvf \"/content/drive/MyDrive/Project 6 - Dataset/saved_models.zip\"\n",
        "\n",
        "if resume_model and not os.path.isfile(\"/content/saved_models/sphereface_model.pth\"):\n",
        "  if use_mount:\n",
        "    !jar xvf \"/content/drive/MyDrive/Project 6 - Dataset/saved_models.zip\"\n",
        "\n",
        "if resume_model and not os.path.isfile(\"/content/saved_models/cosface_model.pth\"):\n",
        "  if use_mount:\n",
        "    !jar xvf \"/content/drive/MyDrive/Project 6 - Dataset/saved_models.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g6SkCgyhl-g"
      },
      "source": [
        "# Download Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFrIIf6E0UQM"
      },
      "source": [
        "Clone of original repo of CosPlace and our code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63SgJ_Y0hwrC",
        "outputId": "fed6492f-479d-403e-bd7f-433482628191"
      },
      "outputs": [],
      "source": [
        "# download code of CosPlace\n",
        "!git clone \"https://github.com/gmberton/CosPlace\" \n",
        "#!rm -r \"/content/CosPlace\"\n",
        "\n",
        "# download our code\n",
        "!git clone --single-branch --branch \"develop\" \"https://github.com/gab-palmeri/aml-geolocalization.git\"\n",
        "!mv \"/content/aml-geolocalization/\" \"/content/Team\"\n",
        "#!rm -r \"/content/aml-geolocalization\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiwWzjzSio-h"
      },
      "source": [
        "\n",
        "\n",
        "# Import Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAd54tr_cNtO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import logging\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import torchvision.transforms as T\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "sys.path.append(\"/content/CosPlace/\")\n",
        "sys.path.append(\"/content/Team/\")\n",
        "import CosPlace\n",
        "from CosPlace import *\n",
        "\n",
        "torch.backends.cudnn.benchmark = True  # Provides a speedup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MsFj6mmaYJs"
      },
      "source": [
        "This class let us to access to dictionary keys like `dict.key` instead of `dict[\"key\"]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKoLeQ_1xGAA"
      },
      "outputs": [],
      "source": [
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb9otl0RvDI3"
      },
      "source": [
        "Drive related functions to save and load training checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcM7DpMovDI3",
        "outputId": "8d521311-3cd9-4ac0-fae9-07c4c60336d5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def drive_save_checkpoint(output_folder, time):\n",
        "    drive_folder = \"/content/drive/MyDrive/project6/\"\n",
        "    if not os.path.exists(drive_folder):\n",
        "        !mkdir \"/content/drive/MyDrive/project6/\"\n",
        "    !zip -r \"/content/drive/MyDrive/project6/{time}.zip\" \"/content/{output_folder}\"\n",
        "\n",
        "def drive_load_checkpoint(input_folder, time):\n",
        "    drive_folder = \"/content/drive/MyDrive/project6/\"\n",
        "    !jar xvf \"/content/drive/MyDrive/project6/{time}.zip\"\n",
        "\n",
        "def drive_tester(output_folder):\n",
        "    drive_folder = \"/content/drive/MyDrive/project6/\"\n",
        "    if not os.path.exists(drive_folder):\n",
        "        !mkdir \"/content/drive/MyDrive/project6/\"\n",
        "    !touch \"/content/drive/MyDrive/project6/test\"\n",
        "\n",
        "    if os.path.exists(\"/content/drive/MyDrive/project6/test\"):\n",
        "        logging.info(\"Drive saving works\")\n",
        "    else:\n",
        "        logging.info(\"WARNING: Drive saving does not work\")\n",
        "\n",
        "# use this function when content on drive are not updated\n",
        "def drive_refresh():\n",
        "  drive.flush_and_unmount()\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpbRF7uZxGAB"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_UnO_6a0amb"
      },
      "source": [
        "Original code provides two main executable files: `train.py` and `eval.py`. We want to reproduce the same behaviour of these two files so these are the parameters passed via terminal. They will be put in a dict called `args` to simply reuse code inside ex files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tOcIQ1Fwni7"
      },
      "source": [
        "## Setup parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI0BfY2_xGAB"
      },
      "outputs": [],
      "source": [
        "# CosPlace Groups parameters\n",
        "COS_M = 10\n",
        "ALPHA = 30\n",
        "COS_N = 5\n",
        "COS_L = 2\n",
        "GROUPS_NUM = 1\n",
        "MIN_IMAGES_PER_CLASS = 10\n",
        "# Model parameters\n",
        "BACKBONE = \"convnext_tiny\"   \n",
        "# [\"resnet18\", \"efficientnet_b2\", \"efficientnet_v2_s\", \"mobilenet_v3_small\", \"mobilenet_v3_large\",\n",
        "# [\"convnext_tiny\", \"swin_tiny\" ]\n",
        "FC_OUTPUT_DIM = 512     # Output dimension of final fully connected layer\n",
        "PRETRAIN = \"imagenet\"   # [\"imagenet\", \"places\", \"gldv2\"]\n",
        "# Training parameters\n",
        "AUGMENTATION_DEVICE = \"cuda\"    # [\"cuda\", \"cpu\"]\n",
        "USE_AMP_16 = AUGMENTATION_DEVICE == \"cuda\"       # use Automatic Mixed Precision\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_NUM = 3\n",
        "ITERATIONS_PER_EPOCH = 10_000\n",
        "LR = 0.00001                    # Learning rate  \n",
        "CLASSIFIERS_LR = 0.01\n",
        "# Data augmentation\n",
        "RESIZE_AS_DB = False\n",
        "BRIGHTNESS = 0.7\n",
        "CONTRAST = 0.7\n",
        "SATURATION = 0.7\n",
        "HUE = 0.5\n",
        "RANDOM_RESIZED_CROP = 0.5\n",
        "RANDOM_H_FLIP = False\n",
        "MAJORITY_WEIGHT = 0.01\n",
        "# Validation / test parameters\n",
        "INFER_BATCH_SIZE = 16           # Batch size for inference (validating and testing)\n",
        "POSITIVE_DIST_THRESHOLD = 25    # distance in meters for a prediction to be considered a positive\n",
        "# Resume parameters\n",
        "RESUME_TRAIN = None     # path to checkpoint to resume, e.g. logs/.../last_checkpoint.pth\n",
        "RESUME_MODEL = None     # Path to model to resume training from\n",
        "# Other parameters\n",
        "DEVICE = \"cuda\"                     # [\"cuda\", \"cpu\"]\n",
        "SEED = 0\n",
        "NUM_WORKERS = 8\n",
        "DATASET_FOLDER = \"/content/small\"   # path of the folder with train/val sets\n",
        "SAVEDIR = BACKBONE + \"_\" + PRETRAIN\n",
        "TEST_METHOD = None\n",
        "\n",
        "\n",
        "if not os.path.exists(DATASET_FOLDER):\n",
        "    raise FileNotFoundError(f\"Dataset folder {DATASET_FOLDER} not found\")\n",
        "\n",
        "train_set_folder = os.path.join(DATASET_FOLDER, \"train\")\n",
        "\n",
        "if not os.path.exists(train_set_folder):\n",
        "    raise FileNotFoundError(f\"Train set folder {train_set_folder} not found\")\n",
        "\n",
        "val_set_folder = os.path.join(DATASET_FOLDER, \"val\")\n",
        "\n",
        "if not os.path.exists(val_set_folder):\n",
        "    raise FileNotFoundError(f\"Validation set folder {val_set_folder} not found\")\n",
        "\n",
        "if BACKBONE != \"resnet18\" and PRETRAIN != \"imagenet\":\n",
        "    raise ValueError(\"Only resnet18 can be pretrained on other datasets than ImageNet\")\n",
        "\n",
        "# dictionary for the parameters\n",
        "args = {\n",
        "    'M': COS_M, 'alpha': ALPHA, 'N': COS_N, 'L': COS_L, 'groups_num': GROUPS_NUM,\n",
        "    'min_images_per_class': MIN_IMAGES_PER_CLASS, 'backbone': BACKBONE, \"pretrain\": PRETRAIN,\n",
        "    'fc_output_dim': FC_OUTPUT_DIM, 'use_amp16': USE_AMP_16,\n",
        "    'augmentation_device': AUGMENTATION_DEVICE, 'batch_size': BATCH_SIZE,\n",
        "    'epochs_num': EPOCHS_NUM, 'iterations_per_epoch': ITERATIONS_PER_EPOCH,\n",
        "    'lr': LR, 'classifiers_lr': CLASSIFIERS_LR, 'brightness': BRIGHTNESS, 'resize_as_db': RESIZE_AS_DB,\n",
        "    'contrast': CONTRAST, 'hue': HUE, 'saturation': SATURATION, 'hflip': RANDOM_H_FLIP, 'maj_weight': MAJORITY_WEIGHT,\n",
        "    'random_resized_crop': RANDOM_RESIZED_CROP, 'infer_batch_size': INFER_BATCH_SIZE,\n",
        "    'positive_dist_threshold': POSITIVE_DIST_THRESHOLD, 'resume_train': RESUME_TRAIN,\n",
        "    'resume_model': RESUME_MODEL, 'device': DEVICE, 'seed': SEED,\n",
        "    'num_workers': NUM_WORKERS, 'dataset_folder': DATASET_FOLDER, 'save_dir': SAVEDIR,\n",
        "    'val_set_folder': val_set_folder, 'train_set_folder': train_set_folder, 'test_method': TEST_METHOD\n",
        "}\n",
        "\n",
        "# this helps to reuse the code from the original CosPlace\n",
        "args = dotdict(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "saveDirLocal = SAVEDIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weAbHrt3PsA3"
      },
      "source": [
        "# Setup logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF4OSAc0PsA3",
        "outputId": "9a3b8b28-b974-47e8-d29e-16ecaf7200e9"
      },
      "outputs": [],
      "source": [
        "from CosPlace import commons\n",
        "\n",
        "start_time = datetime.now()\n",
        "output_folder = f\"logs/{args.save_dir}/{start_time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
        "commons.make_deterministic(args.seed)\n",
        "commons.setup_logging(output_folder, console=None)\n",
        "logging.info(\" \".join(sys.argv))\n",
        "logging.info(f\"Arguments: {args}\")\n",
        "logging.info(f\"The outputs are being saved in {output_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_21_ceryD9G"
      },
      "source": [
        "# PAY ATTENTION!\n",
        "If you want to just test an already provided model, you should skip the training part and go to [Test section](#scrollTo=Y5jJf7v5Ox91)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbHjiIS7xGAC"
      },
      "source": [
        "# Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxEATeZ0PsA3"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmFpttOdPsA3"
      },
      "outputs": [],
      "source": [
        "from Team.model import network\n",
        "\n",
        "model = network.GeoLocalizationNet(args.backbone, args.fc_output_dim, args.pretrain)\n",
        "\n",
        "logging.info(f\"There are {torch.cuda.device_count()} GPUs and {multiprocessing.cpu_count()} CPUs.\")\n",
        "\n",
        "if args.resume_model is not None:\n",
        "    logging.debug(f\"Loading model from {args.resume_model}\")\n",
        "    model_state_dict = torch.load(args.resume_model)\n",
        "    model.load_state_dict(model_state_dict)\n",
        "\n",
        "model = model.to(args.device).train()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iYVmm3_wPsA3"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdqXRnxIPsA3"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "model_optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LIlc-xSPsA4"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CosFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from CosPlace import datasets\n",
        "from datasets.train_dataset import TrainDataset\n",
        "from Team.datasets import DataAugTestDataset\n",
        "from Team.loss.cosface_loss import MarginCosineProduct\n",
        "\n",
        "groups = [TrainDataset(args, args.train_set_folder, M=args.M, alpha=args.alpha, N=args.N, L=args.L,\n",
        "                       current_group=n, min_images_per_class=args.min_images_per_class) for n in range(args.groups_num)]\n",
        "\n",
        "# apply cosface loss to each group\n",
        "# Each group has its own classifier, which depends on the number of classes in the group\n",
        "classifiers = [MarginCosineProduct(args.fc_output_dim, len(group)) for group in groups]\n",
        "classifiers_optimizers = [torch.optim.Adam(classifier.parameters(), lr=args.classifiers_lr) for classifier in classifiers]\n",
        "\n",
        "logging.info(f\"Using {len(groups)} groups\")\n",
        "logging.info(f\"The {len(groups)} groups have respectively the following number of classes {[len(g) for g in groups]}\")\n",
        "logging.info(f\"The {len(groups)} groups have respectively the following number of images {[g.get_images_num() for g in groups]}\")\n",
        "\n",
        "val_ds = DataAugTestDataset(args.val_set_folder, positive_dist_threshold=args.positive_dist_threshold)\n",
        "logging.info(f\"Validation set: {val_ds}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNuFuBpNPsA4"
      },
      "source": [
        "## Resume train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxJEyKRzPsA4"
      },
      "outputs": [],
      "source": [
        "from CosPlace import util\n",
        "\n",
        "if args.resume_train:\n",
        "    model, model_optimizer, classifiers, classifiers_optimizers, best_val_recall1, start_epoch_num = \\\n",
        "        util.resume_train(args, output_folder, model, model_optimizer, classifiers, classifiers_optimizers)\n",
        "    model = model.to(args.device)\n",
        "    epoch_num = start_epoch_num - 1\n",
        "    logging.info(f\"Resuming from epoch {start_epoch_num} with best R@1 {best_val_recall1:.1f} from checkpoint {args.resume_train}\")\n",
        "else:\n",
        "    best_val_recall1 = start_epoch_num = 0\n",
        "    logging.info(\"Starting from scratch, without resuming from a checkpoint\")\n",
        "\n",
        "drive_tester(output_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJnmjudfPsA4"
      },
      "source": [
        "## Train and Evaluation Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRWSj33fPsA4",
        "outputId": "7df6e794-46e4-4f77-a039-ac21f74cd4fb"
      },
      "outputs": [],
      "source": [
        "from CosPlace import augmentations\n",
        "from Team import test as team_test\n",
        "\n",
        "logging.info(\"Start training ...\")\n",
        "logging.info(f\"There are {len(groups[0])} classes for the first group, \" +\n",
        "             f\"each epoch has {args.iterations_per_epoch} iterations \" +\n",
        "             f\"with batch_size {args.batch_size}, therefore the model sees each class (on average) \" +\n",
        "             f\"{args.iterations_per_epoch * args.batch_size / len(groups[0]):.1f} times per epoch\")\n",
        "\n",
        "hflip_trans = T.RandomHorizontalFlip() if args.hflip else T.Identity()\n",
        "\n",
        "if args.augmentation_device == \"cuda\":\n",
        "    gpu_augmentation = T.Compose([\n",
        "            augmentations.DeviceAgnosticColorJitter(brightness=args.brightness,\n",
        "                                                    contrast=args.contrast,\n",
        "                                                    saturation=args.saturation,\n",
        "                                                    hue=args.hue),\n",
        "            augmentations.DeviceAgnosticRandomResizedCrop([512, 512],\n",
        "                                                          scale=[1-args.random_resized_crop, 1]),\n",
        "            hflip_trans,                                                        \n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "if args.use_amp16:\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "for epoch_num in range(start_epoch_num, args.epochs_num):\n",
        "    \n",
        "    #### Train\n",
        "    epoch_start_time = datetime.now()\n",
        "    # Select classifier and dataloader according to epoch\n",
        "    current_group_num = epoch_num % args.groups_num\n",
        "    classifiers[current_group_num] = classifiers[current_group_num].to(args.device)\n",
        "    util.move_to_device(classifiers_optimizers[current_group_num], args.device)\n",
        "    \n",
        "    dataloader = commons.InfiniteDataLoader(groups[current_group_num], num_workers=args.num_workers,\n",
        "                                            batch_size=args.batch_size, shuffle=True,\n",
        "                                            pin_memory=(args.device == \"cuda\"), drop_last=True)\n",
        "    \n",
        "    dataloader_iterator = iter(dataloader)\n",
        "    model = model.train()\n",
        "    \n",
        "    epoch_losses = np.zeros((0, 1), dtype=np.float32)\n",
        "    for iteration in tqdm(range(args.iterations_per_epoch), ncols=100):\n",
        "        images, targets, _ = next(dataloader_iterator)\n",
        "        images, targets = images.to(args.device), targets.to(args.device)\n",
        "        \n",
        "        if args.augmentation_device == \"cuda\":\n",
        "            images = gpu_augmentation(images)\n",
        "        \n",
        "\n",
        "        model_optimizer.zero_grad()\n",
        "        classifiers_optimizers[current_group_num].zero_grad()\n",
        "        \n",
        "        if not args.use_amp16:\n",
        "            descriptors = model(images)\n",
        "            output = classifiers[current_group_num](descriptors, targets)\n",
        "            loss = criterion(output, targets)\n",
        "            loss.backward()\n",
        "            epoch_losses = np.append(epoch_losses, loss.item())\n",
        "            del loss, output, images\n",
        "            model_optimizer.step()\n",
        "            classifiers_optimizers[current_group_num].step()\n",
        "        else:  # Use AMP 16\n",
        "            with torch.cuda.amp.autocast():\n",
        "                descriptors = model(images)\n",
        "                output = classifiers[current_group_num](descriptors, targets)\n",
        "                loss = criterion(output, targets)\n",
        "            scaler.scale(loss).backward()\n",
        "            epoch_losses = np.append(epoch_losses, loss.item())\n",
        "            del loss, output, images\n",
        "            scaler.step(model_optimizer)\n",
        "            scaler.step(classifiers_optimizers[current_group_num])\n",
        "            scaler.update()\n",
        "    \n",
        "    classifiers[current_group_num] = classifiers[current_group_num].cpu()\n",
        "    util.move_to_device(classifiers_optimizers[current_group_num], \"cpu\")\n",
        "    \n",
        "    logging.debug(f\"Epoch {epoch_num:02d} in {str(datetime.now() - epoch_start_time)[:-7]}, \"\n",
        "                  f\"loss = {epoch_losses.mean():.4f}\")\n",
        "    \n",
        "    #### Evaluation\n",
        "    recalls, recalls_str = team_test.test(args, val_ds, model)\n",
        "    logging.info(f\"Epoch {epoch_num:02d} in {str(datetime.now() - epoch_start_time)[:-7]}, {val_ds}: {recalls_str[:20]}\")\n",
        "    is_best = recalls[0] > best_val_recall1\n",
        "    best_val_recall1 = max(recalls[0], best_val_recall1)\n",
        "    # Save checkpoint, which contains all training parameters\n",
        "    util.save_checkpoint({\n",
        "        \"epoch_num\": epoch_num + 1,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": model_optimizer.state_dict(),\n",
        "        \"classifiers_state_dict\": [c.state_dict() for c in classifiers],\n",
        "        \"optimizers_state_dict\": [c.state_dict() for c in classifiers_optimizers],\n",
        "        \"best_val_recall1\": best_val_recall1\n",
        "    }, is_best, output_folder)\n",
        "\n",
        "    # save on drive\n",
        "    drive_save_checkpoint(output_folder, start_time)\n",
        "\n",
        "\n",
        "logging.info(f\"Trained for {epoch_num+1:02d} epochs, in total in {str(datetime.now() - start_time)[:-7]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5jJf7v5Ox91"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ItObSaBl6ox"
      },
      "source": [
        "## Import model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# default is the one just trained\n",
        "resume_model = f\"{output_folder}/best_model.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzJGjw9xxGAE",
        "outputId": "47627e4e-c4a2-42fb-8cb4-9f959c28c2ab"
      },
      "outputs": [],
      "source": [
        "\n",
        "from CosPlace import datasets\n",
        "from Team import test as team_test\n",
        "from Team.model import network\n",
        "from Team.datasets.test_dataset import DataAugTestDataset\n",
        "\n",
        "#### Model\n",
        "model = network.GeoLocalizationNet(args.backbone, args.fc_output_dim, args.pretrain)\n",
        "\n",
        "logging.info(f\"There are {torch.cuda.device_count()} GPUs and {multiprocessing.cpu_count()} CPUs.\")\n",
        "\n",
        "if args.resume_model is not None:\n",
        "  if os.path.exists(args.resume_model):\n",
        "    resume_model = args.resume_model\n",
        "\n",
        "if resume_model is not None:\n",
        "  if not os.path.exists(resume_model):\n",
        "    raise FileNotFoundError(f\"Model {resume_model} not found.\")\n",
        "\n",
        "  logging.info(f\"Loading model from {resume_model}\")\n",
        "  model_state_dict = torch.load(resume_model)\n",
        "  model.load_state_dict(model_state_dict)\n",
        "else:\n",
        "    logging.info(\"WARNING: You didn't provide a path to resume the model (--resume_model parameter). \" +\n",
        "                 \"Evaluation will be computed using randomly initialized weights.\")\n",
        "\n",
        "model = model.to(args.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVEznMFwxGAE"
      },
      "source": [
        "## Test on SF-XS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuyMy5pCweIZ"
      },
      "source": [
        "Test the model on the sf-xs (test) dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XZjk3Z2O1m6",
        "outputId": "b57b3565-b272-47a6-fee5-1c72cf819b06"
      },
      "outputs": [],
      "source": [
        "# dataset_folder is the same of the training\n",
        "test_set_folder = os.path.join(DATASET_FOLDER, \"test\")\n",
        "if not os.path.exists(test_set_folder):\n",
        "    raise FileNotFoundError(f\"Test set folder {test_set_folder} not found\")\n",
        "\n",
        "test_ds = DataAugTestDataset(test_set_folder, queries_folder=\"queries_v1\",\n",
        "                      positive_dist_threshold=args.positive_dist_threshold, test_method = args.test_method, resize=args.resize_as_db)\n",
        "\n",
        "recalls, recalls_str = team_test.test(args, test_ds, model, args.test_method)\n",
        "logging.info(f\"{test_ds}: {recalls_str}\")\n",
        "\n",
        "# recalls for csv file\n",
        "sf_xs_r15 = f\"{recalls[0]:.1f}/{recalls[1]:.1f}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCUwEkPHSpD7"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q28MxTcweIZ"
      },
      "source": [
        "## Test on Tokyo-XS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdmjVczqweIZ"
      },
      "source": [
        "Test the model on the tokyo-xs dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJgmqoCgPxwu",
        "outputId": "7fefb978-b97a-48ed-d0d5-b7bf5c157df8"
      },
      "outputs": [],
      "source": [
        "tokyo_xs_folder = \"/content/tokyo_xs\"\n",
        "test_set_folder = os.path.join(tokyo_xs_folder, \"test\")\n",
        "if not os.path.exists(test_set_folder):\n",
        "    raise FileNotFoundError(f\"Test set folder {test_set_folder} not found\")\n",
        "\n",
        "test_ds = DataAugTestDataset(test_set_folder,\n",
        "                      positive_dist_threshold=args.positive_dist_threshold, test_method = args.test_method, resize=args.resize_as_db)\n",
        "\n",
        "recalls, recalls_str = team_test.test(args, test_ds, model, args.test_method)\n",
        "logging.info(f\"{test_ds}: {recalls_str}\")\n",
        "\n",
        "# recalls for csv file\n",
        "tokyo_xs_r15 = f\"{recalls[0]:.1f}/{recalls[1]:.1f}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0tHKhzeweIZ"
      },
      "source": [
        "## Test on Tokyo-Night"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTc8pkpuweIa"
      },
      "source": [
        "Test the model on the tokyo-night dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9Vzh7YNQrQa",
        "outputId": "521f7df9-34cf-4536-e6e5-1d333cd0190f"
      },
      "outputs": [],
      "source": [
        "tokyo_night_folder = \"/content/tokyo-night/\"\n",
        "test_set_folder = os.path.join(tokyo_night_folder, \"test\")\n",
        "if not os.path.exists(test_set_folder):\n",
        "    raise FileNotFoundError(f\"Test set folder {test_set_folder} not found\")\n",
        "\n",
        "test_ds = DataAugTestDataset(test_set_folder,\n",
        "                      positive_dist_threshold=args.positive_dist_threshold, test_method = args.test_method, resize=args.resize_as_db)\n",
        "\n",
        "recalls, recalls_str = team_test.test(args, test_ds, model, args.test_method)\n",
        "logging.info(f\"{test_ds}: {recalls_str}\")\n",
        "\n",
        "# recalls for csv file\n",
        "tokyo_night_r15 = f\"{recalls[0]:.1f}/{recalls[1]:.1f}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vPCrdJInVZ8"
      },
      "source": [
        "# Save results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cN8-2dhweIa"
      },
      "source": [
        "## Create CSV with recalls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_zxkamqncCt",
        "outputId": "ff92c417-5cb7-4d08-9d5c-a2785cd5dfcf"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "header = [\"sf-xs (test)\", \"Tokyo-xs\", \"Tokyo-night\"]\n",
        "data = [sf_xs_r15, tokyo_xs_r15, tokyo_night_r15]\n",
        "\n",
        "for h, d in zip(header, data):\n",
        "  logging.info(f\"{h}: {d}\")\n",
        "\n",
        "with open(f\"/content/{SAVEDIR}_{start_time.strftime('%Y-%m-%d_%H-%M-%S')}.csv\", \"w\") as f:\n",
        "  writer = csv.writer(f)\n",
        "\n",
        "  writer.writerow(header)\n",
        "  writer.writerow(data)\n",
        "logging.info(f\"save table results to /content/{SAVEDIR}{start_time.strftime('%Y-%m-%d_%H-%M-%S')}.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQXOjFvsweIa"
      },
      "source": [
        "## Save on Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "modelLocal = SAVEDIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPIfX1yp_dtj"
      },
      "source": [
        "Save all data generated by this notebook in a specific folder in **PERSONAL** gdrive. Remember to copy inside shared_data of project drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2mrXPt72snx"
      },
      "source": [
        "### Run the following if you did training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnPzKaCQ-xRD",
        "outputId": "d14a4b99-f809-4c73-cc11-c62d08341fa8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "if not os.path.exists(\"/content/drive/MyDrive/project6\"):\n",
        "  !mkdir /content/drive/MyDrive/project6\n",
        "!cp \"/content/logs/{SAVEDIR}/{start_time.strftime('%Y-%m-%d_%H-%M-%S')}/best_model.pth\" \"/content/drive/MyDrive/project6/{modelLocal}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp-qylkt2yp7"
      },
      "source": [
        "### Run always"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyLTpYMh2iYG",
        "outputId": "b4c3cbdc-7b54-4006-b888-0846190ca262"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# zip logs -> logs.zip\n",
        "!zip -r /content/drive/MyDrive/project_6/logs.zip /content/logs/\n",
        "# zip cache -> cache.zip\n",
        "if os.path.exists(\"/content/cache/\"):\n",
        "  !zip -r /content/drive/MyDrive/project_6/cache.zip /content/cache/\n",
        "\n",
        "!cp \"/content/{SAVEDIR}_{start_time.strftime('%Y-%m-%d_%H-%M-%S')}.csv\" /content/drive/MyDrive/project6/\n",
        "\n",
        "drive.flush_and_unmount()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "qf3z5SqWZ91b"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8 (default, Apr 13 2021, 12:59:45) \n[Clang 10.0.0 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
