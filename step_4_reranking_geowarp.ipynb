{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gab-palmeri/aml-geolocalization/blob/develop/step_4_reranking_geowarp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n18u6OFZxF_z"
      },
      "source": [
        "The purpose of this notebook is to train the model with arcface and sphereface loss function and test with our databases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf3z5SqWZ91b"
      },
      "source": [
        "# pip install requirements\n",
        "\n",
        "Remember to click on \"Restart Runtime\" before go on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YbtEmI1AiTkF",
        "outputId": "84b52b4e-6c61-4716-e689-cf7ab6b9c250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faiss_cpu>=1.7.1\n",
            "  Downloading faiss_cpu-1.7.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.0 MB 27.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Pillow>=9.0.1\n",
            "  Downloading Pillow-9.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 21.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: Pillow\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "Successfully installed Pillow-9.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit_learn>=1.0.2 in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn>=1.0.2) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit_learn>=1.0.2) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit_learn>=1.0.2) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn>=1.0.2) (1.7.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch>=1.8.2 in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.2) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision>=0.9.2 in /usr/local/lib/python3.8/dist-packages (0.14.0+cu116)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.9.2) (2.23.0)\n",
            "Requirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.9.2) (1.13.0+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.9.2) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.9.2) (4.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.9.2) (9.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.9.2) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.9.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.9.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.9.2) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting utm>=0.7.0\n",
            "  Downloading utm-0.7.0.tar.gz (8.7 kB)\n",
            "Building wheels for collected packages: utm\n",
            "  Building wheel for utm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for utm: filename=utm-0.7.0-py3-none-any.whl size=6108 sha256=c5904a8c59c44936388c77ea9d152aa2efb5b6cef42a717a656cf5b94b048f5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/e2/d8/878a8cc986641056fbfebefc4d8eb64238a7b6d3426e86b447\n",
            "Successfully built utm\n",
            "Installing collected packages: utm\n",
            "Successfully installed utm-0.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kornia\n",
            "  Downloading kornia-0.6.9-py2.py3-none-any.whl (569 kB)\n",
            "\u001b[K     |████████████████████████████████| 569 kB 36.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.8/dist-packages (from kornia) (1.13.0+cu116)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from kornia) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.9.1->kornia) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->kornia) (3.0.9)\n",
            "Installing collected packages: kornia\n",
            "Successfully installed kornia-0.6.9\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "# CosPlace requirements\n",
        "!pip3 install \"faiss_cpu>=1.7.1\"\n",
        "!pip3 install \"numpy>=1.21.2\"\n",
        "!pip3 install \"Pillow>=9.0.1\"\n",
        "!pip3 install \"scikit_learn>=1.0.2\"\n",
        "!pip3 install \"torch>=1.8.2\"\n",
        "!pip3 install \"torchvision>=0.9.2\"\n",
        "!pip3 install \"tqdm>=4.62.3\"\n",
        "!pip3 install \"utm>=0.7.0\"\n",
        "!pip3 install \"kornia\"\n",
        "\n",
        "import torch\n",
        "#use GPU if available \n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #'cpu' # 'cuda' or 'cpu'\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czjvnq3FjBmh"
      },
      "source": [
        "# Download Datasets and previous data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hY8bYPGyqLF"
      },
      "source": [
        "Downloading with gdown doesn't work properly.\n",
        "\n",
        "Prefer always to use drive / mount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGOhXMNqjMed",
        "outputId": "3d5755e1-74f5-4be1-d055-1508afd90f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "mkdir: cannot create directory ‘/content/saved_models’: File exists\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gdown\n",
        "from google.colab import drive\n",
        "\n",
        "def download(id, output=None, quiet=True):\n",
        "  gdown.download(\n",
        "    f\"https://drive.google.com/uc?export=download&confirm=pbef&id={id}\",\n",
        "    output=output,\n",
        "    quiet=quiet\n",
        "  )\n",
        "\n",
        "\n",
        "use_mount = True\n",
        "resume_model = True\n",
        "\n",
        "if use_mount:\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "# TOKYO-XS DATASET\n",
        "if not os.path.isdir(\"/content/tokyo_xs\"):\n",
        "  if use_mount:\n",
        "    !jar xvf \"/content/drive/MyDrive/Project 6 - Dataset/tokyo-xs.zip\"\n",
        "  else:\n",
        "    id = \"1fBCnap5BRh36474cVkjvjlC-yUTEb1n3\"\n",
        "    download(id, quiet=False)                           # download from our gdrive\n",
        "    !jar xvf \"/content/tokyo-xs.zip\"                    # unzip\n",
        "    !rm -r \"/content/tokyo-xs.zip\"                      # remove .zip file\n",
        "\n",
        "if not os.path.isdir(\"/content/tokyo_xs\"):\n",
        "  raise FileNotFoundError(f\"Can't download tokyo xs\")\n",
        "\n",
        "#TOKYO NIGHT DATASET\n",
        "if not os.path.isdir(\"/content/tokyo-night\"):\n",
        "  if use_mount:\n",
        "    !jar xvf \"/content/drive/MyDrive/Project 6 - Dataset/tokyo-night.zip\"\n",
        "  else:\n",
        "    id = \"1EZJY2r5565-iVk2oEbTns0xc4F_em4Y4\"\n",
        "    download(id, quiet=False)                           # download from our gdrive\n",
        "    !jar xvf \"/content/tokyo-night.zip\"                    # unzip\n",
        "    !rm -r \"/content/tokyo-night.zip\"\n",
        "\n",
        "if not os.path.isdir(\"/content/tokyo-night\"):\n",
        "  raise FileNotFoundError(f\"Can't download tokyo night\")\n",
        "\n",
        "# SAN FRANCISCO - XS DATASET\n",
        "if not os.path.isdir(\"/content/small\"):\n",
        "  if use_mount:\n",
        "    !jar xvf \"/content/drive/MyDrive/Project 6 - Dataset/sf-xs.zip\"\n",
        "  else:\n",
        "    id = \"1brIxBJmOgvuzFbI57f5LxnMxjccUu993\"\n",
        "    download(id, quiet=False)                           # download\n",
        "    !jar xvf \"/content/sf-xs.zip\"                       # unzip\n",
        "    !rm -r \"/content/sf-xs.zip\"                         # remove .zip file\n",
        "\n",
        "if not os.path.isdir(\"/content/small\"):\n",
        "  raise FileNotFoundError(f\"Can't download sfxs\")\n",
        "\n",
        "if resume_model and not os.path.isfile(\"/content/saved_models/cosface_model.pth\"):\n",
        "  if use_mount:\n",
        "    !mkdir \"/content/saved_models\"\n",
        "    !cp \"/content/drive/MyDrive/step_4_rerank/cosface_model.pth\" \"/content/saved_models\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g6SkCgyhl-g"
      },
      "source": [
        "# Download Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFrIIf6E0UQM"
      },
      "source": [
        "Clone of original repo of CosPlace and our code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63SgJ_Y0hwrC",
        "outputId": "821663e6-0569-4a1d-f089-83242f45531e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'CosPlace' already exists and is not an empty directory.\n",
            "fatal: destination path 'geo_warp' already exists and is not an empty directory.\n",
            "Cloning into 'aml-geolocalization'...\n",
            "remote: Enumerating objects: 151, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 151 (delta 24), reused 51 (delta 22), pack-reused 96\u001b[K\n",
            "Receiving objects: 100% (151/151), 1.96 MiB | 40.04 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n"
          ]
        }
      ],
      "source": [
        "# download code of CosPlace\n",
        "!git clone \"https://github.com/gmberton/CosPlace\" \n",
        "#!rm -r \"/content/CosPlace\"\n",
        "\n",
        "# download code of GeoWarp\n",
        "!git clone \"https://github.com/gmberton/geo_warp\"\n",
        "\n",
        "# download our code\n",
        "#!rm -rf Team\n",
        "!git clone --single-branch --branch \"develop\" \"https://github.com/gab-palmeri/aml-geolocalization.git\"\n",
        "!mv \"/content/aml-geolocalization/\" \"/content/Team\"\n",
        "#!rm -r \"/content/aml-geolocalization\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiwWzjzSio-h"
      },
      "source": [
        "\n",
        "\n",
        "# Import Code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iAd54tr_cNtO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import logging\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import torchvision.transforms as T\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "sys.path.append(\"/content/CosPlace/\")\n",
        "sys.path.append(\"/content/geo_warp/\")\n",
        "sys.path.append(\"/content/Team/\")\n",
        "import CosPlace\n",
        "from CosPlace import *\n",
        "\n",
        "import Team.GeoWarp as GeoWarp\n",
        "from GeoWarp import *\n",
        "from Team import *\n",
        "\n",
        "torch.backends.cudnn.benchmark = True  # Provides a speedup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MsFj6mmaYJs"
      },
      "source": [
        "This class let us to access to dictionary keys like `dict.key` instead of `dict[\"key\"]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PKoLeQ_1xGAA"
      },
      "outputs": [],
      "source": [
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb9otl0RvDI3"
      },
      "source": [
        "Drive related functions to save and load training checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcM7DpMovDI3",
        "outputId": "f83b4e10-d1c3-4335-bc93-888f6a969a5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def drive_save_checkpoint(output_folder, time):\n",
        "    drive_folder = \"/content/drive/MyDrive/project6/\"\n",
        "    if not os.path.exists(drive_folder):\n",
        "        !mkdir \"/content/drive/MyDrive/project6/\"\n",
        "    !zip -r \"/content/drive/MyDrive/project6/{time}.zip\" \"/content/{output_folder}\"\n",
        "\n",
        "def drive_load_checkpoint(input_folder, time):\n",
        "    drive_folder = \"/content/drive/MyDrive/project6/\"\n",
        "    !jar xvf \"/content/drive/MyDrive/project6/{time}.zip\"\n",
        "\n",
        "def drive_tester(output_folder):\n",
        "    drive_folder = \"/content/drive/MyDrive/project6/\"\n",
        "    if not os.path.exists(drive_folder):\n",
        "        !mkdir \"/content/drive/MyDrive/project6/\"\n",
        "    !touch \"/content/drive/MyDrive/project6/test\"\n",
        "\n",
        "    if os.path.exists(\"/content/drive/MyDrive/project6/test\"):\n",
        "        logging.info(\"Drive saving works\")\n",
        "    else:\n",
        "        logging.info(\"WARNING: Drive saving does not work\")\n",
        "\n",
        "# use this function when content on drive are not updated\n",
        "def drive_refresh():\n",
        "  drive.flush_and_unmount()\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpbRF7uZxGAB"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_UnO_6a0amb"
      },
      "source": [
        "Original code provides two main executable files: `train.py` and `eval.py`. We want to reproduce the same behaviour of these two files so these are the parameters passed via terminal. They will be put in a dict called `args` to simply reuse code inside ex files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EETNlk73f5Sm"
      },
      "source": [
        "### Save dir for CosFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FIZU5KjLf5Sm"
      },
      "outputs": [],
      "source": [
        "saveDirLocal = \"cosface\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tOcIQ1Fwni7"
      },
      "source": [
        "## Setup parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sI0BfY2_xGAB"
      },
      "outputs": [],
      "source": [
        "# CosPlace Groups parameters\n",
        "COS_M = 10\n",
        "ALPHA = 30\n",
        "COS_N = 5\n",
        "COS_L = 2\n",
        "GROUPS_NUM = 1\n",
        "MIN_IMAGES_PER_CLASS = 10\n",
        "# Model parameters\n",
        "BACKBONE = \"resnet18\"   # [\"vgg16\", \"resnet18\", \"resnet50\", \"resnet101\", \"resnet152\"]\n",
        "FC_OUTPUT_DIM = 512     # Output dimension of final fully connected layer\n",
        "# Training parameters\n",
        "USE_AMP_16 = \"store_true\"       # use Automatic Mixed Precision\n",
        "AUGMENTATION_DEVICE = \"cuda\"    # [\"cuda\", \"cpu\"]\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_NUM = 3\n",
        "ITERATIONS_PER_EPOCH = 10_000\n",
        "LR = 0.00001                    # Learning rate  \n",
        "CLASSIFIERS_LR = 0.01\n",
        "# Data augmentation\n",
        "BRIGHTNESS = 0.7\n",
        "CONTRAST = 0.7\n",
        "SATURATION = 0.7\n",
        "HUE = 0.5\n",
        "RANDOM_RESIZED_CROP = 0.5\n",
        "# Validation / test parameters\n",
        "INFER_BATCH_SIZE = 16           # Batch size for inference (validating and testing)\n",
        "POSITIVE_DIST_THRESHOLD = 25    # distance in meters for a prediction to be considered a positive\n",
        "# Resume parameters\n",
        "RESUME_TRAIN = None     # path to checkpoint to resume, e.g. logs/.../last_checkpoint.pth\n",
        "RESUME_MODEL = None     # Path to model to resume training from\n",
        "# Other parameters\n",
        "DEVICE = \"cuda\"                     # [\"cuda\", \"cpu\"]\n",
        "SEED = 0\n",
        "NUM_WORKERS = 8\n",
        "DATASET_FOLDER = \"/content/small\"   # path of the folder with train/val sets\n",
        "SAVEDIR = saveDirLocal\n",
        "\n",
        "\n",
        "if not os.path.exists(DATASET_FOLDER):\n",
        "    raise FileNotFoundError(f\"Dataset folder {DATASET_FOLDER} not found\")\n",
        "\n",
        "train_set_folder = os.path.join(DATASET_FOLDER, \"train\")\n",
        "\n",
        "if not os.path.exists(train_set_folder):\n",
        "    raise FileNotFoundError(f\"Train set folder {train_set_folder} not found\")\n",
        "\n",
        "val_set_folder = os.path.join(DATASET_FOLDER, \"val\")\n",
        "\n",
        "if not os.path.exists(val_set_folder):\n",
        "    raise FileNotFoundError(f\"Validation set folder {val_set_folder} not found\")\n",
        "\n",
        "\n",
        "# dictionary for the parameters\n",
        "args = {\n",
        "    'M': COS_M, 'alpha': ALPHA, 'N': COS_N, 'L': COS_L, 'groups_num': GROUPS_NUM,\n",
        "    'min_images_per_class': MIN_IMAGES_PER_CLASS, 'backbone': BACKBONE,\n",
        "    'fc_output_dim': FC_OUTPUT_DIM, 'use_amp_16': USE_AMP_16,\n",
        "    'augmentation_device': AUGMENTATION_DEVICE, 'batch_size': BATCH_SIZE,\n",
        "    'epochs_num': EPOCHS_NUM, 'iterations_per_epoch': ITERATIONS_PER_EPOCH,\n",
        "    'lr': LR, 'classifiers_lr': CLASSIFIERS_LR, 'brightness': BRIGHTNESS,\n",
        "    'contrast': CONTRAST, 'hue': HUE, 'saturation': SATURATION,\n",
        "    'random_resized_crop': RANDOM_RESIZED_CROP, 'infer_batch_size': INFER_BATCH_SIZE,\n",
        "    'positive_dist_threshold': POSITIVE_DIST_THRESHOLD, 'resume_train': RESUME_TRAIN,\n",
        "    'resume_model': RESUME_MODEL, 'device': DEVICE, 'seed': SEED,\n",
        "    'num_workers': NUM_WORKERS, 'dataset_folder': DATASET_FOLDER, 'save_dir': SAVEDIR,\n",
        "    'val_set_folder': val_set_folder, 'train_set_folder': train_set_folder,\n",
        "}\n",
        "\n",
        "# this helps to reuse the code from the original CosPlace\n",
        "args = dotdict(args)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weAbHrt3PsA3"
      },
      "source": [
        "# Setup logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF4OSAc0PsA3",
        "outputId": "7ac7b146-b4f5-4ccf-846b-e8a6c9fddfa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py -f /root/.local/share/jupyter/runtime/kernel-c2f10f1c-54ca-424c-8b16-53a564ce741a.json\n",
            "INFO:root:Arguments: {'M': 10, 'alpha': 30, 'N': 5, 'L': 2, 'groups_num': 1, 'min_images_per_class': 10, 'backbone': 'resnet18', 'fc_output_dim': 512, 'use_amp_16': 'store_true', 'augmentation_device': 'cuda', 'batch_size': 32, 'epochs_num': 3, 'iterations_per_epoch': 10000, 'lr': 1e-05, 'classifiers_lr': 0.01, 'brightness': 0.7, 'contrast': 0.7, 'hue': 0.5, 'saturation': 0.7, 'random_resized_crop': 0.5, 'infer_batch_size': 16, 'positive_dist_threshold': 25, 'resume_train': None, 'resume_model': None, 'device': 'cuda', 'seed': 0, 'num_workers': 8, 'dataset_folder': '/content/small', 'save_dir': 'cosface', 'val_set_folder': '/content/small/val', 'train_set_folder': '/content/small/train'}\n",
            "INFO:root:The outputs are being saved in logs/cosface/2022-12-27_17-38-42\n"
          ]
        }
      ],
      "source": [
        "from CosPlace import commons\n",
        "\n",
        "start_time = datetime.now()\n",
        "output_folder = f\"logs/{args.save_dir}/{start_time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
        "commons.make_deterministic(args.seed)\n",
        "commons.setup_logging(output_folder, console=None)\n",
        "logging.info(\" \".join(sys.argv))\n",
        "logging.info(f\"Arguments: {args}\")\n",
        "logging.info(f\"The outputs are being saved in {output_folder}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_21_ceryD9G"
      },
      "source": [
        "# PAY ATTENTION!\n",
        "If you want to just test an already provided model, you should skip the training part and go to [Test section](#scrollTo=Y5jJf7v5Ox91)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training GeoWarp"
      ],
      "metadata": {
        "id": "nDtASz-Niqo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/Team/GeoWarp/train.py --arch resnet50 --pooling gem --resume_fe /content/saved_models/cosface_model.pth"
      ],
      "metadata": {
        "id": "gQ-N2CPjitM_",
        "outputId": "536a863c-0cfc-4794-9c9e-10f979a3b7fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-27 17:24:30   python /content/Team/GeoWarp/train.py --arch resnet50 --pooling gem --resume_fe /content/saved_models/cosface_model.pth\n",
            "2022-12-27 17:24:30   Arguments: Namespace(arch='resnet50', batch_size_consistency=16, batch_size_features_wise=16, batch_size_ss=16, channels=[225, 128, 128, 64, 64, 64, 64], consistency_w=0.1, dataset_name='sf-xs', datasets_folder='/content/small', exp_name='default', features_wise_w=10, iterations_per_epoch=500, k=0.6, kernel_sizes=[7, 5, 5, 5, 5, 5], lr=0.0001, n_epochs=3, num_reranked_preds=20, optim='sgd', pooling='gem', positive_dist_threshold=25, qp_num_workers=4, qp_threshold=1.2, resume_fe='/content/saved_models/cosface_model.pth', ss_num_workers=8, ss_w=1)\n",
            "2022-12-27 17:24:30   The outputs are being saved in runs/default/2022-12-27_17-24-30\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100% 97.8M/97.8M [00:02<00:00, 49.3MB/s]\n",
            "2022-12-27 17:24:37   \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Team/GeoWarp/train.py\", line 132, in <module>\n",
            "    features_extractor.load_state_dict(state_dict)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1667, in load_state_dict\n",
            "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
            "RuntimeError: Error(s) in loading state_dict for FeaturesExtractor:\n",
            "\tMissing key(s) in state_dict: \"encoder.0.weight\", \"encoder.1.weight\", \"encoder.1.bias\", \"encoder.1.running_mean\", \"encoder.1.running_var\", \"encoder.4.0.conv1.weight\", \"encoder.4.0.bn1.weight\", \"encoder.4.0.bn1.bias\", \"encoder.4.0.bn1.running_mean\", \"encoder.4.0.bn1.running_var\", \"encoder.4.0.conv2.weight\", \"encoder.4.0.bn2.weight\", \"encoder.4.0.bn2.bias\", \"encoder.4.0.bn2.running_mean\", \"encoder.4.0.bn2.running_var\", \"encoder.4.0.conv3.weight\", \"encoder.4.0.bn3.weight\", \"encoder.4.0.bn3.bias\", \"encoder.4.0.bn3.running_mean\", \"encoder.4.0.bn3.running_var\", \"encoder.4.0.downsample.0.weight\", \"encoder.4.0.downsample.1.weight\", \"encoder.4.0.downsample.1.bias\", \"encoder.4.0.downsample.1.running_mean\", \"encoder.4.0.downsample.1.running_var\", \"encoder.4.1.conv1.weight\", \"encoder.4.1.bn1.weight\", \"encoder.4.1.bn1.bias\", \"encoder.4.1.bn1.running_mean\", \"encoder.4.1.bn1.running_var\", \"encoder.4.1.conv2.weight\", \"encoder.4.1.bn2.weight\", \"encoder.4.1.bn2.bias\", \"encoder.4.1.bn2.running_mean\", \"encoder.4.1.bn2.running_var\", \"encoder.4.1.conv3.weight\", \"encoder.4.1.bn3.weight\", \"encoder.4.1.bn3.bias\", \"encoder.4.1.bn3.running_mean\", \"encoder.4.1.bn3.running_var\", \"encoder.4.2.conv1.weight\", \"encoder.4.2.bn1.weight\", \"encoder.4.2.bn1.bias\", \"encoder.4.2.bn1.running_mean\", \"encoder.4.2.bn1.running_var\", \"encoder.4.2.conv2.weight\", \"encoder.4.2.bn2.weight\", \"encoder.4.2.bn2.bias\", \"encoder.4.2.bn2.running_mean\", \"encoder.4.2.bn2.running_var\", \"encoder.4.2.conv3.weight\", \"encoder.4.2.bn3.weight\", \"encoder.4.2.bn3.bias\", \"encoder.4.2.bn3.running_mean\", \"encoder.4.2.bn3.running_var\", \"encoder.5.0.conv1.weight\", \"encoder.5.0.bn1.weight\", \"encoder.5.0.bn1.bias\", \"encoder.5.0.bn1.running_mean\", \"encoder.5.0.bn1.running_var\", \"encoder.5.0.conv2.weight\", \"encoder.5.0.bn2.weight\", \"encoder.5.0.bn2.bias\", \"encoder.5.0.bn2.running_mean\", \"encoder.5.0.bn2.running_var\", \"encoder.5.0.conv3.weight\", \"encoder.5.0.bn3.weight\", \"encoder.5.0.bn3.bias\", \"encoder.5.0.bn3.running_mean\", \"encoder.5.0.bn3.running_var\", \"encoder.5.0.downsample.0.weight\", \"encoder.5.0.downsample.1.weight\", \"encoder.5.0.downsample.1.bias\", \"encoder.5.0.downsample.1.running_mean\", \"encoder.5.0.downsample.1.running_var\", \"encoder.5.1.conv1.weight\", \"encoder.5.1.bn1.weight\", \"encoder.5.1.bn1.bias\", \"encoder.5.1.bn1.running_mean\", \"encoder.5.1.bn1.running_var\", \"encoder.5.1.conv2.weight\", \"encoder.5.1.bn2.weight\", \"encoder.5.1.bn2.bias\", \"encoder.5.1.bn2.running_mean\", \"encoder.5.1.bn2.running_var\", \"encoder.5.1.conv3.weight\", \"encoder.5.1.bn3.weight\", \"encoder.5.1.bn3.bias\", \"encoder.5.1.bn3.running_mean\", \"encoder.5.1.bn3.running_var\", \"encoder.5.2.conv1.weight\", \"encoder.5.2.bn1.weight\", \"encoder.5.2.bn1.bias\", \"encoder.5.2.bn1.running_mean\", \"encoder.5.2.bn1.running_var\", \"encoder.5.2.conv2.weight\", \"encoder.5.2.bn2.weight\", \"encoder.5.2.bn2.bias\", \"encoder.5.2.bn2.running_mean\", \"encoder.5.2.bn2.running_var\", \"encoder.5.2.conv3.weight\", \"encoder.5.2.bn3.weight\", \"encoder.5.2.bn3.bias\", \"encoder.5.2.bn3.running_mean\", \"encoder.5.2.bn3.running_var\", \"encoder.5.3.conv1.weight\", \"encoder.5.3.bn1.weight\", \"encoder.5.3.bn1.bias\", \"encoder.5.3.bn1.running_mean\", \"encoder.5.3.bn1.running_var\", \"encoder.5.3.conv2.weight\", \"encoder.5.3.bn2.weight\", \"encoder.5.3.bn2.bias\", \"encoder.5.3.bn2.running_mean\", \"encoder.5.3.bn2.running_var\", \"encoder.5.3.conv3.weight\", \"encoder.5.3.bn3.weight\", \"encoder.5.3.bn3.bias\", \"encoder.5.3.bn3.running_mean\", \"encoder.5.3.bn3.running_var\", \"encoder.6.0.conv1.weight\", \"encoder.6.0.bn1.weight\", \"encoder.6.0.bn1.bias\", \"encoder.6.0.bn1.running_mean\", \"encoder.6.0.bn1.running_var\", \"encoder.6.0.conv2.weight\", \"encoder.6.0.bn2.weight\", \"encoder.6.0.bn2.bias\", \"encoder.6.0.bn2.running_mean\", \"encoder.6.0.bn2.running_var\", \"encoder.6.0.conv3.weight\", \"encoder.6.0.bn3.weight\", \"encoder.6.0.bn3.bias\", \"encoder.6.0.bn3.running_mean\", \"encoder.6.0.bn3.running_var\", \"encoder.6.0.downsample.0.weight\", \"encoder.6.0.downsample.1.weight\", \"encoder.6.0.downsample.1.bias\", \"encoder.6.0.downsample.1.running_mean\", \"encoder.6.0.downsample.1.running_var\", \"encoder.6.1.conv1.weight\", \"encoder.6.1.bn1.weight\", \"encoder.6.1.bn1.bias\", \"encoder.6.1.bn1.running_mean\", \"encoder.6.1.bn1.running_var\", \"encoder.6.1.conv2.weight\", \"encoder.6.1.bn2.weight\", \"encoder.6.1.bn2.bias\", \"encoder.6.1.bn2.running_mean\", \"encoder.6.1.bn2.running_var\", \"encoder.6.1.conv3.weight\", \"encoder.6.1.bn3.weight\", \"encoder.6.1.bn3.bias\", \"encoder.6.1.bn3.running_mean\", \"encoder.6.1.bn3.running_var\", \"encoder.6.2.conv1.weight\", \"encoder.6.2.bn1.weight\", \"encoder.6.2.bn1.bias\", \"encoder.6.2.bn1.running_mean\", \"encoder.6.2.bn1.running_var\", \"encoder.6.2.conv2.weight\", \"encoder.6.2.bn2.weight\", \"encoder.6.2.bn2.bias\", \"encoder.6.2.bn2.running_mean\", \"encoder.6.2.bn2.running_var\", \"encoder.6.2.conv3.weight\", \"encoder.6.2.bn3.weight\", \"encoder.6.2.bn3.bias\", \"encoder.6.2.bn3.running_mean\", \"encoder.6.2.bn3.running_var\", \"encoder.6.3.conv1.weight\", \"encoder.6.3.bn1.weight\", \"encoder.6.3.bn1.bias\", \"encoder.6.3.bn1.running_mean\", \"encoder.6.3.bn1.running_var\", \"encoder.6.3.conv2.weight\", \"encoder.6.3.bn2.weight\", \"encoder.6.3.bn2.bias\", \"encoder.6.3.bn2.running_mean\", \"encoder.6.3.bn2.running_var\", \"encoder.6.3.conv3.weight\", \"encoder.6.3.bn3.weight\", \"encoder.6.3.bn3.bias\", \"encoder.6.3.bn3.running_mean\", \"encoder.6.3.bn3.running_var\", \"encoder.6.4.conv1.weight\", \"encoder.6.4.bn1.weight\", \"encoder.6.4.bn1.bias\", \"encoder.6.4.bn1.running_mean\", \"encoder.6.4.bn1.running_var\", \"encoder.6.4.conv2.weight\", \"encoder.6.4.bn2.weight\", \"encoder.6.4.bn2.bias\", \"encoder.6.4.bn2.running_mean\", \"encoder.6.4.bn2.running_var\", \"encoder.6.4.conv3.weight\", \"encoder.6.4.bn3.weight\", \"encoder.6.4.bn3.bias\", \"encoder.6.4.bn3.running_mean\", \"encoder.6.4.bn3.running_var\", \"encoder.6.5.conv1.weight\", \"encoder.6.5.bn1.weight\", \"encoder.6.5.bn1.bias\", \"encoder.6.5.bn1.running_mean\", \"encoder.6.5.bn1.running_var\", \"encoder.6.5.conv2.weight\", \"encoder.6.5.bn2.weight\", \"encoder.6.5.bn2.bias\", \"encoder.6.5.bn2.running_mean\", \"encoder.6.5.bn2.running_var\", \"encoder.6.5.conv3.weight\", \"encoder.6.5.bn3.weight\", \"encoder.6.5.bn3.bias\", \"encoder.6.5.bn3.running_mean\", \"encoder.6.5.bn3.running_var\", \"pool.1.p\". \n",
            "\tUnexpected key(s) in state_dict: \"backbone.0.weight\", \"backbone.1.weight\", \"backbone.1.bias\", \"backbone.1.running_mean\", \"backbone.1.running_var\", \"backbone.1.num_batches_tracked\", \"backbone.4.0.conv1.weight\", \"backbone.4.0.bn1.weight\", \"backbone.4.0.bn1.bias\", \"backbone.4.0.bn1.running_mean\", \"backbone.4.0.bn1.running_var\", \"backbone.4.0.bn1.num_batches_tracked\", \"backbone.4.0.conv2.weight\", \"backbone.4.0.bn2.weight\", \"backbone.4.0.bn2.bias\", \"backbone.4.0.bn2.running_mean\", \"backbone.4.0.bn2.running_var\", \"backbone.4.0.bn2.num_batches_tracked\", \"backbone.4.1.conv1.weight\", \"backbone.4.1.bn1.weight\", \"backbone.4.1.bn1.bias\", \"backbone.4.1.bn1.running_mean\", \"backbone.4.1.bn1.running_var\", \"backbone.4.1.bn1.num_batches_tracked\", \"backbone.4.1.conv2.weight\", \"backbone.4.1.bn2.weight\", \"backbone.4.1.bn2.bias\", \"backbone.4.1.bn2.running_mean\", \"backbone.4.1.bn2.running_var\", \"backbone.4.1.bn2.num_batches_tracked\", \"backbone.5.0.conv1.weight\", \"backbone.5.0.bn1.weight\", \"backbone.5.0.bn1.bias\", \"backbone.5.0.bn1.running_mean\", \"backbone.5.0.bn1.running_var\", \"backbone.5.0.bn1.num_batches_tracked\", \"backbone.5.0.conv2.weight\", \"backbone.5.0.bn2.weight\", \"backbone.5.0.bn2.bias\", \"backbone.5.0.bn2.running_mean\", \"backbone.5.0.bn2.running_var\", \"backbone.5.0.bn2.num_batches_tracked\", \"backbone.5.0.downsample.0.weight\", \"backbone.5.0.downsample.1.weight\", \"backbone.5.0.downsample.1.bias\", \"backbone.5.0.downsample.1.running_mean\", \"backbone.5.0.downsample.1.running_var\", \"backbone.5.0.downsample.1.num_batches_tracked\", \"backbone.5.1.conv1.weight\", \"backbone.5.1.bn1.weight\", \"backbone.5.1.bn1.bias\", \"backbone.5.1.bn1.running_mean\", \"backbone.5.1.bn1.running_var\", \"backbone.5.1.bn1.num_batches_tracked\", \"backbone.5.1.conv2.weight\", \"backbone.5.1.bn2.weight\", \"backbone.5.1.bn2.bias\", \"backbone.5.1.bn2.running_mean\", \"backbone.5.1.bn2.running_var\", \"backbone.5.1.bn2.num_batches_tracked\", \"backbone.6.0.conv1.weight\", \"backbone.6.0.bn1.weight\", \"backbone.6.0.bn1.bias\", \"backbone.6.0.bn1.running_mean\", \"backbone.6.0.bn1.running_var\", \"backbone.6.0.bn1.num_batches_tracked\", \"backbone.6.0.conv2.weight\", \"backbone.6.0.bn2.weight\", \"backbone.6.0.bn2.bias\", \"backbone.6.0.bn2.running_mean\", \"backbone.6.0.bn2.running_var\", \"backbone.6.0.bn2.num_batches_tracked\", \"backbone.6.0.downsample.0.weight\", \"backbone.6.0.downsample.1.weight\", \"backbone.6.0.downsample.1.bias\", \"backbone.6.0.downsample.1.running_mean\", \"backbone.6.0.downsample.1.running_var\", \"backbone.6.0.downsample.1.num_batches_tracked\", \"backbone.6.1.conv1.weight\", \"backbone.6.1.bn1.weight\", \"backbone.6.1.bn1.bias\", \"backbone.6.1.bn1.running_mean\", \"backbone.6.1.bn1.running_var\", \"backbone.6.1.bn1.num_batches_tracked\", \"backbone.6.1.conv2.weight\", \"backbone.6.1.bn2.weight\", \"backbone.6.1.bn2.bias\", \"backbone.6.1.bn2.running_mean\", \"backbone.6.1.bn2.running_var\", \"backbone.6.1.bn2.num_batches_tracked\", \"backbone.7.0.conv1.weight\", \"backbone.7.0.bn1.weight\", \"backbone.7.0.bn1.bias\", \"backbone.7.0.bn1.running_mean\", \"backbone.7.0.bn1.running_var\", \"backbone.7.0.bn1.num_batches_tracked\", \"backbone.7.0.conv2.weight\", \"backbone.7.0.bn2.weight\", \"backbone.7.0.bn2.bias\", \"backbone.7.0.bn2.running_mean\", \"backbone.7.0.bn2.running_var\", \"backbone.7.0.bn2.num_batches_tracked\", \"backbone.7.0.downsample.0.weight\", \"backbone.7.0.downsample.1.weight\", \"backbone.7.0.downsample.1.bias\", \"backbone.7.0.downsample.1.running_mean\", \"backbone.7.0.downsample.1.running_var\", \"backbone.7.0.downsample.1.num_batches_tracked\", \"backbone.7.1.conv1.weight\", \"backbone.7.1.bn1.weight\", \"backbone.7.1.bn1.bias\", \"backbone.7.1.bn1.running_mean\", \"backbone.7.1.bn1.running_var\", \"backbone.7.1.bn1.num_batches_tracked\", \"backbone.7.1.conv2.weight\", \"backbone.7.1.bn2.weight\", \"backbone.7.1.bn2.bias\", \"backbone.7.1.bn2.running_mean\", \"backbone.7.1.bn2.running_var\", \"backbone.7.1.bn2.num_batches_tracked\", \"aggregation.1.p\", \"aggregation.3.weight\", \"aggregation.3.bias\". \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbHjiIS7xGAC"
      },
      "source": [
        "# Training CosPlace\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxEATeZ0PsA3"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmFpttOdPsA3"
      },
      "outputs": [],
      "source": [
        "from CosPlace import model\n",
        "from model import network\n",
        "\n",
        "model = network.GeoLocalizationNet(args.backbone, args.fc_output_dim)\n",
        "\n",
        "logging.info(f\"There are {torch.cuda.device_count()} GPUs and {multiprocessing.cpu_count()} CPUs.\")\n",
        "\n",
        "if args.resume_model is not None:\n",
        "    logging.debug(f\"Loading model from {args.resume_model}\")\n",
        "    model_state_dict = torch.load(args.resume_model)\n",
        "    model.load_state_dict(model_state_dict)\n",
        "\n",
        "model = model.to(args.device).train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYVmm3_wPsA3"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdqXRnxIPsA3"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "model_optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LIlc-xSPsA4"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-IC1LeSf5Sr"
      },
      "source": [
        "### CosFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hvaahti-f5Sr"
      },
      "outputs": [],
      "source": [
        "from CosPlace import datasets\n",
        "from datasets.train_dataset import TrainDataset\n",
        "from datasets.test_dataset import TestDataset\n",
        "from Team.loss.cosface import CosFace\n",
        "\n",
        "groups = [TrainDataset(args, args.train_set_folder, M=args.M, alpha=args.alpha, N=args.N, L=args.L,\n",
        "                       current_group=n, min_images_per_class=args.min_images_per_class) for n in range(args.groups_num)]\n",
        "\n",
        "# apply cosface loss to each group\n",
        "# Each group has its own classifier, which depends on the number of classes in the group\n",
        "classifiers = [CosFace(args.fc_output_dim, len(group)) for group in groups]\n",
        "classifiers_optimizers = [torch.optim.Adam(classifier.parameters(), lr=args.classifiers_lr) for classifier in classifiers]\n",
        "\n",
        "logging.info(f\"Using {len(groups)} groups\")\n",
        "logging.info(f\"The {len(groups)} groups have respectively the following number of classes {[len(g) for g in groups]}\")\n",
        "logging.info(f\"The {len(groups)} groups have respectively the following number of images {[g.get_images_num() for g in groups]}\")\n",
        "\n",
        "val_ds = TestDataset(args.val_set_folder, positive_dist_threshold=args.positive_dist_threshold)\n",
        "logging.info(f\"Validation set: {val_ds}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNuFuBpNPsA4"
      },
      "source": [
        "## Resume train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxJEyKRzPsA4"
      },
      "outputs": [],
      "source": [
        "from CosPlace import util\n",
        "\n",
        "if args.resume_train:\n",
        "    model, model_optimizer, classifiers, classifiers_optimizers, best_val_recall1, start_epoch_num = \\\n",
        "        util.resume_train(args, output_folder, model, model_optimizer, classifiers, classifiers_optimizers)\n",
        "    model = model.to(args.device)\n",
        "    epoch_num = start_epoch_num - 1\n",
        "    logging.info(f\"Resuming from epoch {start_epoch_num} with best R@1 {best_val_recall1:.1f} from checkpoint {args.resume_train}\")\n",
        "else:\n",
        "    best_val_recall1 = start_epoch_num = 0\n",
        "\n",
        "drive_tester(output_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJnmjudfPsA4"
      },
      "source": [
        "## Train and Evaluation Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRWSj33fPsA4",
        "outputId": "7df6e794-46e4-4f77-a039-ac21f74cd4fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
            "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
            "INFO:root:Start training ...\n",
            "INFO:root:There are 5965 classes for the first group, each epoch has 10000 iterations with batch_size 32, therefore the model sees each class (on average) 53.6 times per epoch\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "100%|███████████████████████████████████████████████████████| 10000/10000 [1:14:13<00:00,  2.25it/s]\n",
            "DEBUG:root:Epoch 00 in 1:14:14, loss = 7.9503\n",
            "DEBUG:root:Extracting database descriptors for evaluation/testing\n",
            "100%|█████████████████████████████████████████████████████████████| 501/501 [01:05<00:00,  7.64it/s]\n",
            "DEBUG:root:Extracting queries descriptors for evaluation/testing using batch size 1\n",
            "100%|███████████████████████████████████████████████████████████| 7993/7993 [01:49<00:00, 72.98it/s]\n",
            "DEBUG:root:Calculating recalls\n",
            "INFO:root:Epoch 00 in 1:17:10, < val - #q: 7993; #db: 8015 >: R@1: 78.7, R@5: 88.0\n",
            "100%|███████████████████████████████████████████████████████| 10000/10000 [1:14:16<00:00,  2.24it/s]\n",
            "DEBUG:root:Epoch 01 in 1:14:17, loss = 3.3677\n",
            "DEBUG:root:Extracting database descriptors for evaluation/testing\n",
            "100%|█████████████████████████████████████████████████████████████| 501/501 [01:04<00:00,  7.73it/s]\n",
            "DEBUG:root:Extracting queries descriptors for evaluation/testing using batch size 1\n",
            "100%|███████████████████████████████████████████████████████████| 7993/7993 [01:51<00:00, 71.48it/s]\n",
            "DEBUG:root:Calculating recalls\n",
            "INFO:root:Epoch 01 in 1:17:15, < val - #q: 7993; #db: 8015 >: R@1: 81.8, R@5: 89.9\n",
            "100%|███████████████████████████████████████████████████████| 10000/10000 [1:14:11<00:00,  2.25it/s]\n",
            "DEBUG:root:Epoch 02 in 1:14:12, loss = 2.4285\n",
            "DEBUG:root:Extracting database descriptors for evaluation/testing\n",
            "100%|█████████████████████████████████████████████████████████████| 501/501 [01:03<00:00,  7.89it/s]\n",
            "DEBUG:root:Extracting queries descriptors for evaluation/testing using batch size 1\n",
            "100%|███████████████████████████████████████████████████████████| 7993/7993 [01:49<00:00, 73.33it/s]\n",
            "DEBUG:root:Calculating recalls\n",
            "INFO:root:Epoch 02 in 1:17:06, < val - #q: 7993; #db: 8015 >: R@1: 83.2, R@5: 90.6\n",
            "INFO:root:Trained for 03 epochs, in total in 3:53:58\n"
          ]
        }
      ],
      "source": [
        "from CosPlace import augmentations, test\n",
        "\n",
        "logging.info(\"Start training ...\")\n",
        "logging.info(f\"There are {len(groups[0])} classes for the first group, \" +\n",
        "             f\"each epoch has {args.iterations_per_epoch} iterations \" +\n",
        "             f\"with batch_size {args.batch_size}, therefore the model sees each class (on average) \" +\n",
        "             f\"{args.iterations_per_epoch * args.batch_size / len(groups[0]):.1f} times per epoch\")\n",
        "\n",
        "\n",
        "if args.augmentation_device == \"cuda\":\n",
        "    gpu_augmentation = T.Compose([\n",
        "            augmentations.DeviceAgnosticColorJitter(brightness=args.brightness,\n",
        "                                                    contrast=args.contrast,\n",
        "                                                    saturation=args.saturation,\n",
        "                                                    hue=args.hue),\n",
        "            augmentations.DeviceAgnosticRandomResizedCrop([512, 512],\n",
        "                                                          scale=[1-args.random_resized_crop, 1]),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "if args.use_amp16:\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "for epoch_num in range(start_epoch_num, args.epochs_num):\n",
        "    \n",
        "    #### Train\n",
        "    epoch_start_time = datetime.now()\n",
        "    # Select classifier and dataloader according to epoch\n",
        "    current_group_num = epoch_num % args.groups_num\n",
        "    classifiers[current_group_num] = classifiers[current_group_num].to(args.device)\n",
        "    util.move_to_device(classifiers_optimizers[current_group_num], args.device)\n",
        "    \n",
        "    dataloader = commons.InfiniteDataLoader(groups[current_group_num], num_workers=args.num_workers,\n",
        "                                            batch_size=args.batch_size, shuffle=True,\n",
        "                                            pin_memory=(args.device == \"cuda\"), drop_last=True)\n",
        "    \n",
        "    dataloader_iterator = iter(dataloader)\n",
        "    model = model.train()\n",
        "    \n",
        "    epoch_losses = np.zeros((0, 1), dtype=np.float32)\n",
        "    for iteration in tqdm(range(args.iterations_per_epoch), ncols=100):\n",
        "        images, targets, _ = next(dataloader_iterator)\n",
        "        images, targets = images.to(args.device), targets.to(args.device)\n",
        "        \n",
        "        if args.augmentation_device == \"cuda\":\n",
        "            images = gpu_augmentation(images)\n",
        "        \n",
        "        model_optimizer.zero_grad()\n",
        "        classifiers_optimizers[current_group_num].zero_grad()\n",
        "        \n",
        "        if not args.use_amp16:\n",
        "            descriptors = model(images)\n",
        "            output = classifiers[current_group_num](descriptors, targets)\n",
        "            loss = criterion(output, targets)\n",
        "            loss.backward()\n",
        "            epoch_losses = np.append(epoch_losses, loss.item())\n",
        "            del loss, output, images\n",
        "            model_optimizer.step()\n",
        "            classifiers_optimizers[current_group_num].step()\n",
        "        else:  # Use AMP 16\n",
        "            with torch.cuda.amp.autocast():\n",
        "                descriptors = model(images)\n",
        "                output = classifiers[current_group_num](descriptors, targets)\n",
        "                loss = criterion(output, targets)\n",
        "            scaler.scale(loss).backward()\n",
        "            epoch_losses = np.append(epoch_losses, loss.item())\n",
        "            del loss, output, images\n",
        "            scaler.step(model_optimizer)\n",
        "            scaler.step(classifiers_optimizers[current_group_num])\n",
        "            scaler.update()\n",
        "    \n",
        "    classifiers[current_group_num] = classifiers[current_group_num].cpu()\n",
        "    util.move_to_device(classifiers_optimizers[current_group_num], \"cpu\")\n",
        "    \n",
        "    logging.debug(f\"Epoch {epoch_num:02d} in {str(datetime.now() - epoch_start_time)[:-7]}, \"\n",
        "                  f\"loss = {epoch_losses.mean():.4f}\")\n",
        "    \n",
        "    #### Evaluation\n",
        "    recalls, recalls_str = test.test(args, val_ds, model)\n",
        "    logging.info(f\"Epoch {epoch_num:02d} in {str(datetime.now() - epoch_start_time)[:-7]}, {val_ds}: {recalls_str[:20]}\")\n",
        "    is_best = recalls[0] > best_val_recall1\n",
        "    best_val_recall1 = max(recalls[0], best_val_recall1)\n",
        "    # Save checkpoint, which contains all training parameters\n",
        "    util.save_checkpoint({\n",
        "        \"epoch_num\": epoch_num + 1,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": model_optimizer.state_dict(),\n",
        "        \"classifiers_state_dict\": [c.state_dict() for c in classifiers],\n",
        "        \"optimizers_state_dict\": [c.state_dict() for c in classifiers_optimizers],\n",
        "        \"best_val_recall1\": best_val_recall1\n",
        "    }, is_best, output_folder)\n",
        "\n",
        "    # save on drive\n",
        "    drive_save_checkpoint(output_folder, start_time)\n",
        "\n",
        "\n",
        "logging.info(f\"Trained for {epoch_num+1:02d} epochs, in total in {str(datetime.now() - start_time)[:-7]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5jJf7v5Ox91"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ItObSaBl6ox"
      },
      "source": [
        "## Import model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKK4lqRQf5St"
      },
      "source": [
        "#### CosFace resume model path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GuoUPrapf5St"
      },
      "outputs": [],
      "source": [
        "resume_model = \"/content/saved_models/cosface_model.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248,
          "referenced_widgets": [
            "c2fc8774a1624a8493302d24c3e036d0",
            "af61a7fb06dd4e21a1f76d37ca3df92f",
            "406d75615fb944d79189c5809cdabb3f",
            "337c323edc6043609c4fe5affcfe2c45",
            "3a265e6ec1654f49881ba014d5ebbd70",
            "87b8105ebf474056a79b1cca0587efb4",
            "1924873c029d461fbd2167e2edb7ed27",
            "109b20728e1a41e1befb15f212e6507d",
            "754db634b4be4338b023cb120c2af15a",
            "973860aa4718416cb42b109426fb8b4a",
            "8412e1aa8494409a86ddab1c10b87b8b"
          ]
        },
        "id": "KzJGjw9xxGAE",
        "outputId": "bc49dabf-e923-486a-f382-82f37f700bb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
            "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2fc8774a1624a8493302d24c3e036d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:root:Train only layer3 and layer4 of the resnet18, freeze the previous ones\n",
            "INFO:root:There are 1 GPUs and 2 CPUs.\n",
            "INFO:root:Loading model from /content/saved_models/cosface_model.pth\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from CosPlace import model, test, datasets\n",
        "from model import network\n",
        "from datasets.test_dataset import TestDataset\n",
        "\n",
        "#### Model\n",
        "model = network.GeoLocalizationNet(args.backbone, args.fc_output_dim)\n",
        "\n",
        "logging.info(f\"There are {torch.cuda.device_count()} GPUs and {multiprocessing.cpu_count()} CPUs.\")\n",
        "\n",
        "if args.resume_model is not None:\n",
        "  if os.path.exists(args.resume_model):\n",
        "    resume_model = args.resume_model\n",
        "\n",
        "if resume_model is not None:\n",
        "  if not os.path.exists(resume_model):\n",
        "    raise FileNotFoundError(f\"Model {resume_model} not found.\")\n",
        "\n",
        "  logging.info(f\"Loading model from {resume_model}\")\n",
        "  model_state_dict = torch.load(resume_model)\n",
        "  model.load_state_dict(model_state_dict)\n",
        "else:\n",
        "    logging.info(\"WARNING: You didn't provide a path to resume the model (--resume_model parameter). \" +\n",
        "                 \"Evaluation will be computed using randomly initialized weights.\")\n",
        "\n",
        "model = model.to(args.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVEznMFwxGAE"
      },
      "source": [
        "## Test on SF-XS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuyMy5pCweIZ"
      },
      "source": [
        "Test the model on the sf-xs (test) dataset. WITHOUT GeoWarp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XZjk3Z2O1m6",
        "outputId": "b57b3565-b272-47a6-fee5-1c72cf819b06"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:root:Extracting database descriptors for evaluation/testing\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "100%|███████████████████████████████████████████████████████████| 1700/1700 [03:52<00:00,  7.32it/s]\n",
            "DEBUG:root:Extracting queries descriptors for evaluation/testing using batch size 1\n",
            "100%|███████████████████████████████████████████████████████████| 1000/1000 [00:15<00:00, 62.62it/s]\n",
            "DEBUG:root:Calculating recalls\n",
            "INFO:root:< test - #q: 1000; #db: 27191 >: R@1: 50.4, R@5: 64.4, R@10: 68.9, R@20: 73.5\n"
          ]
        }
      ],
      "source": [
        "# dataset_folder is the same of the training\n",
        "test_set_folder = os.path.join(DATASET_FOLDER, \"test\")\n",
        "if not os.path.exists(test_set_folder):\n",
        "    raise FileNotFoundError(f\"Test set folder {test_set_folder} not found\")\n",
        "\n",
        "test_ds = TestDataset(test_set_folder, queries_folder=\"queries_v1\",\n",
        "                      positive_dist_threshold=args.positive_dist_threshold)\n",
        "\n",
        "recalls, recalls_str = test.test(args, test_ds, model)\n",
        "logging.info(f\"{test_ds}: {recalls_str}\")\n",
        "\n",
        "# recalls for csv file\n",
        "sf_xs_r15 = f\"{recalls[0]:.1f}/{recalls[1]:.1f}\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model on the sf-xs (test) dataset. WITH GeoWarp"
      ],
      "metadata": {
        "id": "E3pY8Y_dqCKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Team import test_with_geowarp as testgeo\n",
        "\n",
        "# dataset_folder is the same of the training\n",
        "test_set_folder = os.path.join(DATASET_FOLDER, \"test\")\n",
        "if not os.path.exists(test_set_folder):\n",
        "    raise FileNotFoundError(f\"Test set folder {test_set_folder} not found\")\n",
        "\n",
        "test_ds = TestDataset(test_set_folder, queries_folder=\"queries_v1\",\n",
        "                      positive_dist_threshold=args.positive_dist_threshold)\n",
        "\n",
        "recalls, recalls_str = testgeo.test(args, test_ds, model)\n",
        "logging.info(f\"{test_ds}: {recalls_str}\")\n",
        "\n",
        "# recalls for csv file\n",
        "sf_xs_r15 = f\"{recalls[0]:.1f}/{recalls[1]:.1f}\""
      ],
      "metadata": {
        "id": "L7IYBLaBqBX9",
        "outputId": "59527dde-eeca-4a52-bd10-549ddf646b78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 891
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:root:Extracting database descriptors for evaluation/testing\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "  1%|▉                                                            | 25/1700 [00:11<13:01,  2.14it/s]\n",
            "Exception in thread Thread-23:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.8/threading.py\", line 870, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 49, in _pin_memory_loop\n",
            "    do_one_step()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 26, in do_one_step\n",
            "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 116, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/multiprocessing/reductions.py\", line 305, in rebuild_storage_fd\n",
            "    fd = df.detach()\n",
            "  File \"/usr/lib/python3.8/multiprocessing/resource_sharer.py\", line 57, in detach\n",
            "    with _resource_sharer.get_connection(self._id) as conn:\n",
            "  File \"/usr/lib/python3.8/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
            "    c = Client(address, authkey=process.current_process().authkey)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 502, in Client\n",
            "    c = SocketClient(address)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 630, in SocketClient\n",
            "    s.connect(address)\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2bafbe7c2c4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                       positive_dist_threshold=args.positive_dist_threshold)\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mrecalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecalls_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestgeo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{test_ds}: {recalls_str}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Team/test_with_geowarp.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(args, eval_ds, model)\u001b[0m\n\u001b[1;32m     42\u001b[0m                                          batch_size=args.infer_batch_size, pin_memory=(args.device == \"cuda\"))\n\u001b[1;32m     43\u001b[0m         \u001b[0mall_descriptors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_output_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mdescriptors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdescriptors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdescriptors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q28MxTcweIZ"
      },
      "source": [
        "## Test on Tokyo-XS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdmjVczqweIZ"
      },
      "source": [
        "Test the model on the tokyo-xs dataset. WITHOUT GeoWarp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJgmqoCgPxwu",
        "outputId": "7fefb978-b97a-48ed-d0d5-b7bf5c157df8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:root:Extracting database descriptors for evaluation/testing\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "100%|█████████████████████████████████████████████████████████████| 799/799 [02:02<00:00,  6.55it/s]\n",
            "DEBUG:root:Extracting queries descriptors for evaluation/testing using batch size 1\n",
            "100%|█████████████████████████████████████████████████████████████| 315/315 [00:05<00:00, 58.72it/s]\n",
            "DEBUG:root:Calculating recalls\n",
            "INFO:root:< test - #q: 315; #db: 12771 >: R@1: 70.8, R@5: 85.7, R@10: 90.5, R@20: 93.0\n"
          ]
        }
      ],
      "source": [
        "tokyo_xs_folder = \"/content/tokyo_xs\"\n",
        "test_set_folder = os.path.join(tokyo_xs_folder, \"test\")\n",
        "if not os.path.exists(test_set_folder):\n",
        "    raise FileNotFoundError(f\"Test set folder {test_set_folder} not found\")\n",
        "\n",
        "test_ds = TestDataset(test_set_folder,\n",
        "                      positive_dist_threshold=args.positive_dist_threshold)\n",
        "\n",
        "recalls, recalls_str = test.test(args, test_ds, model)\n",
        "logging.info(f\"{test_ds}: {recalls_str}\")\n",
        "\n",
        "# recalls for csv file\n",
        "tokyo_xs_r15 = f\"{recalls[0]:.1f}/{recalls[1]:.1f}\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model on the tokyo-xs dataset WITH GeoWarp"
      ],
      "metadata": {
        "id": "IcF9m6Cgq0wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Team import test_with_geowarp as testgeo\n",
        "\n",
        "tokyo_xs_folder = \"/content/tokyo_xs\"\n",
        "test_set_folder = os.path.join(tokyo_xs_folder, \"test\")\n",
        "if not os.path.exists(test_set_folder):\n",
        "    raise FileNotFoundError(f\"Test set folder {test_set_folder} not found\")\n",
        "\n",
        "test_ds = TestDataset(test_set_folder,\n",
        "                      positive_dist_threshold=args.positive_dist_threshold)\n",
        "\n",
        "recalls, recalls_str = testgeo.test(args, test_ds, model)\n",
        "logging.info(f\"{test_ds}: {recalls_str}\")\n",
        "\n",
        "# recalls for csv file\n",
        "tokyo_xs_r15 = f\"{recalls[0]:.1f}/{recalls[1]:.1f}\""
      ],
      "metadata": {
        "id": "ohC0drlhq4-N",
        "outputId": "285d912a-7a8e-459f-f213-db382bb8d3f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:root:Extracting database descriptors for evaluation/testing\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "  1%|▍                                                              | 6/799 [00:03<06:52,  1.92it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-88572a3d8e98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                       positive_dist_threshold=args.positive_dist_threshold)\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mrecalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecalls_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestgeo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{test_ds}: {recalls_str}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Team/test_with_geowarp.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(args, eval_ds, model)\u001b[0m\n\u001b[1;32m     42\u001b[0m                                          batch_size=args.infer_batch_size, pin_memory=(args.device == \"cuda\"))\n\u001b[1;32m     43\u001b[0m         \u001b[0mall_descriptors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_output_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mdescriptors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdescriptors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdescriptors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1203\u001b[0m                     \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_t\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_print_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmininterval\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcur_t\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmin_start_t\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1205\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_print_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1206\u001b[0m                         \u001b[0mlast_print_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_print_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m                         \u001b[0mlast_print_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_print_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1254\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ema_dn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ema_dt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_miniters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m                     \u001b[0;31m# If no `miniters` was specified, adjust automatically to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1507\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1509\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mprint_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisp_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mfp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0mfp_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mlast_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tqdm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0tHKhzeweIZ"
      },
      "source": [
        "## Test on Tokyo-Night"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTc8pkpuweIa"
      },
      "source": [
        "Test the model on the tokyo-night dataset WITHOUT GeoWarp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "H9Vzh7YNQrQa",
        "outputId": "de77fbff-cb0e-4073-bfeb-9515a5961d03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:root:Extracting database descriptors for evaluation/testing\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "  2%|█▍                                                            | 18/799 [00:04<03:21,  3.87it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ce01f50edfe9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                       positive_dist_threshold=args.positive_dist_threshold)\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mrecalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecalls_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestgeo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{test_ds}: {recalls_str}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Team/test_with_geowarp.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(args, eval_ds, model)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mdescriptors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mdescriptors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdescriptors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mall_descriptors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdescriptors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from Team import test_with_geowarp as testgeo\n",
        "\n",
        "tokyo_night_folder = \"/content/tokyo-night/\"\n",
        "test_set_folder = os.path.join(tokyo_night_folder, \"test\")\n",
        "if not os.path.exists(test_set_folder):\n",
        "    raise FileNotFoundError(f\"Test set folder {test_set_folder} not found\")\n",
        "\n",
        "test_ds = TestDataset(test_set_folder,\n",
        "                      positive_dist_threshold=args.positive_dist_threshold)\n",
        "\n",
        "recalls, recalls_str = testgeo.test(args, test_ds, model)\n",
        "logging.info(f\"{test_ds}: {recalls_str}\")\n",
        "\n",
        "# recalls for csv file\n",
        "tokyo_night_r15 = f\"{recalls[0]:.1f}/{recalls[1]:.1f}\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model on the tokyo-night dataset WITH GeoWarp"
      ],
      "metadata": {
        "id": "8VthPxtnrUBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokyo_night_folder = \"/content/tokyo-night/\"\n",
        "test_set_folder = os.path.join(tokyo_night_folder, \"test\")\n",
        "if not os.path.exists(test_set_folder):\n",
        "    raise FileNotFoundError(f\"Test set folder {test_set_folder} not found\")\n",
        "\n",
        "test_ds = TestDataset(test_set_folder,\n",
        "                      positive_dist_threshold=args.positive_dist_threshold)\n",
        "\n",
        "recalls, recalls_str = test.test(args, test_ds, model)\n",
        "logging.info(f\"{test_ds}: {recalls_str}\")\n",
        "\n",
        "# recalls for csv file\n",
        "tokyo_night_r15 = f\"{recalls[0]:.1f}/{recalls[1]:.1f}\""
      ],
      "metadata": {
        "id": "qyAmCBbArYpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vPCrdJInVZ8"
      },
      "source": [
        "# Save results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cN8-2dhweIa"
      },
      "source": [
        "## Create CSV with recalls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_zxkamqncCt",
        "outputId": "ff92c417-5cb7-4d08-9d5c-a2785cd5dfcf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:sf-xs (test): 50.4/64.4\n",
            "INFO:root:Tokyo-xs: 70.8/85.7\n",
            "INFO:root:Tokyo-night: 54.3/72.4\n",
            "INFO:root:save table results to /content/sphereface2022-12-16_21-36-11.csv\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "header = [\"sf-xs (test)\", \"Tokyo-xs\", \"Tokyo-night\"]\n",
        "data = [sf_xs_r15, tokyo_xs_r15, tokyo_night_r15]\n",
        "\n",
        "for h, d in zip(header, data):\n",
        "  logging.info(f\"{h}: {d}\")\n",
        "\n",
        "with open(f\"/content/{SAVEDIR}_{start_time.strftime('%Y-%m-%d_%H-%M-%S')}.csv\", \"w\") as f:\n",
        "  writer = csv.writer(f)\n",
        "\n",
        "  writer.writerow(header)\n",
        "  writer.writerow(data)\n",
        "logging.info(f\"save table results to /content/{SAVEDIR}{start_time.strftime('%Y-%m-%d_%H-%M-%S')}.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQXOjFvsweIa"
      },
      "source": [
        "## Save on Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoE-S18Gf5Sx"
      },
      "source": [
        "#### CosFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjiF_xi5f5Sx"
      },
      "outputs": [],
      "source": [
        "modelLocal = \"cosface_model.pth\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPIfX1yp_dtj"
      },
      "source": [
        "Save all data generated by this notebook in a specific folder in **PERSONAL** gdrive. Remember to copy inside shared_data of project drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2mrXPt72snx"
      },
      "source": [
        "### Run the following if you did training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnPzKaCQ-xRD",
        "outputId": "d14a4b99-f809-4c73-cc11-c62d08341fa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "cp: cannot stat '/content/logs/sphereface/2022-12-16_21-36-11/best_model.pth': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "if not os.path.exists(\"/content/drive/MyDrive/step_3\"):\n",
        "  !mkdir /content/drive/MyDrive/step_3\n",
        "!cp \"/content/logs/{SAVEDIR}/{start_time.strftime('%Y-%m-%d_%H-%M-%S')}/best_model.pth\" \"/content/drive/MyDrive/step_3/{modelLocal}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp-qylkt2yp7"
      },
      "source": [
        "### Run always"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyLTpYMh2iYG",
        "outputId": "b4c3cbdc-7b54-4006-b888-0846190ca262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "updating: content/logs/ (stored 0%)\n",
            "updating: content/logs/arcface/ (stored 0%)\n",
            "updating: content/logs/arcface/2022-12-16_21-06-49/ (stored 0%)\n",
            "updating: content/logs/arcface/2022-12-16_21-06-49/debug.log (deflated 76%)\n",
            "updating: content/logs/arcface/2022-12-16_21-06-49/info.log (deflated 74%)\n",
            "  adding: content/logs/sphereface/ (stored 0%)\n",
            "  adding: content/logs/sphereface/2022-12-16_21-36-11/ (stored 0%)\n",
            "  adding: content/logs/sphereface/2022-12-16_21-36-11/debug.log (deflated 61%)\n",
            "  adding: content/logs/sphereface/2022-12-16_21-36-11/info.log (deflated 53%)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# zip logs -> logs.zip\n",
        "!zip -r /content/drive/MyDrive/step_3/logs.zip /content/logs/\n",
        "# zip cache -> cache.zip\n",
        "if os.path.exists(\"/content/cache/\"):\n",
        "  !zip -r /content/drive/MyDrive/step_3/cache.zip /content/cache/\n",
        "\n",
        "!cp \"/content/{SAVEDIR}_{start_time.strftime('%Y-%m-%d_%H-%M-%S')}.csv\" /content/drive/MyDrive/step_3/\n",
        "\n",
        "drive.flush_and_unmount()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "qf3z5SqWZ91b"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.9 (default, Mar 30 2022, 13:51:16) \n[Clang 13.1.6 (clang-1316.0.21.2.3)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c2fc8774a1624a8493302d24c3e036d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af61a7fb06dd4e21a1f76d37ca3df92f",
              "IPY_MODEL_406d75615fb944d79189c5809cdabb3f",
              "IPY_MODEL_337c323edc6043609c4fe5affcfe2c45"
            ],
            "layout": "IPY_MODEL_3a265e6ec1654f49881ba014d5ebbd70"
          }
        },
        "af61a7fb06dd4e21a1f76d37ca3df92f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87b8105ebf474056a79b1cca0587efb4",
            "placeholder": "​",
            "style": "IPY_MODEL_1924873c029d461fbd2167e2edb7ed27",
            "value": "100%"
          }
        },
        "406d75615fb944d79189c5809cdabb3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_109b20728e1a41e1befb15f212e6507d",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_754db634b4be4338b023cb120c2af15a",
            "value": 46830571
          }
        },
        "337c323edc6043609c4fe5affcfe2c45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_973860aa4718416cb42b109426fb8b4a",
            "placeholder": "​",
            "style": "IPY_MODEL_8412e1aa8494409a86ddab1c10b87b8b",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 68.4MB/s]"
          }
        },
        "3a265e6ec1654f49881ba014d5ebbd70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87b8105ebf474056a79b1cca0587efb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1924873c029d461fbd2167e2edb7ed27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "109b20728e1a41e1befb15f212e6507d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "754db634b4be4338b023cb120c2af15a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "973860aa4718416cb42b109426fb8b4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8412e1aa8494409a86ddab1c10b87b8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}