{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtK2xrTOW8rKV2vd/4XJlV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gab-palmeri/aml-geolocalization/blob/sam/testing_default.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf3z5SqWZ91b"
      },
      "source": [
        "# Torch "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbtEmI1AiTkF",
        "outputId": "5593d4bc-8929-453a-ad70-f48c6af645d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# CosPlace requirements\n",
        "!pip3 install \"faiss_cpu>=1.7.1\"\n",
        "!pip3 install \"numpy>=1.21.2\"\n",
        "!pip3 install \"Pillow>=9.0.1\"\n",
        "!pip3 install \"scikit_learn>=1.0.2\"\n",
        "!pip3 install \"torch>=1.8.2\"\n",
        "!pip3 install \"torchvision>=0.9.2\"\n",
        "!pip3 install \"tqdm>=4.62.3\"\n",
        "!pip3 install \"utm>=0.7.0\"\n",
        "\n",
        "import torch\n",
        "#use GPU if available \n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #'cpu' # 'cuda' or 'cpu'\n",
        "print(DEVICE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faiss_cpu>=1.7.1\n",
            "  Downloading faiss_cpu-1.7.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.0 MB 905 kB/s \n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Pillow>=9.0.1\n",
            "  Downloading Pillow-9.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 15.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: Pillow\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.10 requires torch<1.14,>=1.7, but you have torch 1.4.0 which is incompatible.\n",
            "fastai 2.7.10 requires torchvision>=0.8.2, but you have torchvision 0.5.0 which is incompatible.\u001b[0m\n",
            "Successfully installed Pillow-9.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit_learn>=1.0.2 in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn>=1.0.2) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn>=1.0.2) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit_learn>=1.0.2) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit_learn>=1.0.2) (1.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch>=1.8.2\n",
            "  Downloading torch-1.13.0-cp38-cp38-manylinux1_x86_64.whl (890.2 MB)\n",
            "\u001b[K     |██████████████████████████████  | 834.1 MB 1.2 MB/s eta 0:00:48tcmalloc: large alloc 1147494400 bytes == 0x39722000 @  0x7fcc9a92b615 0x5d631c 0x51e4f1 0x51e67b 0x4f7585 0x49ca7c 0x4fdff5 0x49caa1 0x4fdff5 0x49ced5 0x4f60a9 0x55f926 0x4f60a9 0x55f926 0x4f60a9 0x55f926 0x5d7c18 0x5d9412 0x586636 0x5d813c 0x55f3fd 0x55e571 0x5d7cf1 0x49ced5 0x55e571 0x5d7cf1 0x49ec69 0x5d7c18 0x49ca7c 0x4fdff5 0x49ced5\n",
            "\u001b[K     |████████████████████████████████| 890.2 MB 5.4 kB/s \n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[K     |████████████████████████████████| 849 kB 61.6 MB/s \n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 557.1 MB 11 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.2) (4.1.1)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 317.1 MB 31 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.2) (0.38.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.2) (57.4.0)\n",
            "Installing collected packages: nvidia-cublas-cu11, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.5.0 requires torch==1.4.0, but you have torch 1.13.0 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.13.0 which is incompatible.\n",
            "fastai 2.7.10 requires torchvision>=0.8.2, but you have torchvision 0.5.0 which is incompatible.\u001b[0m\n",
            "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchvision>=0.9.2\n",
            "  Downloading torchvision-0.14.0-cp38-cp38-manylinux1_x86_64.whl (24.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.3 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.9.2) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.9.2) (4.1.1)\n",
            "Requirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.9.2) (1.13.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.9.2) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.9.2) (9.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0->torchvision>=0.9.2) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0->torchvision>=0.9.2) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0->torchvision>=0.9.2) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch==1.13.0->torchvision>=0.9.2) (8.5.0.96)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchvision>=0.9.2) (57.4.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchvision>=0.9.2) (0.38.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.9.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.9.2) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.9.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.9.2) (3.0.4)\n",
            "Installing collected packages: torchvision\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.5.0\n",
            "    Uninstalling torchvision-0.5.0:\n",
            "      Successfully uninstalled torchvision-0.5.0\n",
            "Successfully installed torchvision-0.14.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting utm>=0.7.0\n",
            "  Downloading utm-0.7.0.tar.gz (8.7 kB)\n",
            "Building wheels for collected packages: utm\n",
            "  Building wheel for utm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for utm: filename=utm-0.7.0-py3-none-any.whl size=6108 sha256=3cef0887de0cc62b5c920fd218c87536a567855c7d46de3787fc340cb690c280\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/e2/d8/878a8cc986641056fbfebefc4d8eb64238a7b6d3426e86b447\n",
            "Successfully built utm\n",
            "Installing collected packages: utm\n",
            "Successfully installed utm-0.7.0\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czjvnq3FjBmh"
      },
      "source": [
        "# Download Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGOhXMNqjMed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a087384e-4671-455a-a989-5f08f04959a3"
      },
      "source": [
        "import os\n",
        "\n",
        "# TOKYO DATASET\n",
        "if not os.path.isdir(\"/content/tokyo_xs\"):\n",
        "  !gdown \"1FYcZuawvy42-PTyBl3PI8tKa1cLg__jN\"  # download\n",
        "  !jar xvf \"/content/tokyo-xs.zip\"            # unzip\n",
        "  !rm -r \"/content/tokyo-xs.zip\"              # remove .zip file\n",
        "\n",
        "\n",
        "# SAN FRANCISCO DATASET\n",
        "if not os.path.isdir(\"/content/small\"):\n",
        "  !gdown \"1CQrhB_x9MECtjm0LjasXDxM9N9h24mnz\"  # download\n",
        "  !jar xvf \"/content/sf-xs.zip\"               # unzip\n",
        "  !rm -r \"/content/sf-xs.zip\"                 # remove .zip file"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access denied with the following error:\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1FYcZuawvy42-PTyBl3PI8tKa1cLg__jN \n",
            "\n",
            "java.io.FileNotFoundException: /content/tokyo-xs.zip (No such file or directory)\n",
            "\tat java.base/java.io.FileInputStream.open0(Native Method)\n",
            "\tat java.base/java.io.FileInputStream.open(FileInputStream.java:219)\n",
            "\tat java.base/java.io.FileInputStream.<init>(FileInputStream.java:157)\n",
            "\tat java.base/java.io.FileInputStream.<init>(FileInputStream.java:112)\n",
            "\tat jdk.jartool/sun.tools.jar.Main.run(Main.java:407)\n",
            "\tat jdk.jartool/sun.tools.jar.Main.main(Main.java:1680)\n",
            "rm: cannot remove '/content/tokyo-xs.zip': No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py\", line 377, in _make_request\n",
            "    httplib_response = conn.getresponse(buffering=True)\n",
            "TypeError: getresponse() got an unexpected keyword argument 'buffering'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py\", line 597, in urlopen\n",
            "    httplib_response = self._make_request(conn, method, url,\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py\", line 380, in _make_request\n",
            "    httplib_response = conn.getresponse()\n",
            "  File \"/usr/lib/python3.8/http/client.py\", line 1348, in getresponse\n",
            "    response.begin()\n",
            "  File \"/usr/lib/python3.8/http/client.py\", line 316, in begin\n",
            "    version, status, reason = self._read_status()\n",
            "  File \"/usr/lib/python3.8/http/client.py\", line 277, in _read_status\n",
            "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
            "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/lib/python3.8/ssl.py\", line 1241, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "  File \"/usr/lib/python3.8/ssl.py\", line 1099, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/gdown\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/gdown/cli.py\", line 156, in main\n",
            "    filename = download(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/gdown/download.py\", line 146, in download\n",
            "    res = sess.get(url, headers=headers, stream=True, verify=verify)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/requests/sessions.py\", line 543, in get\n",
            "    return self.request('GET', url, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/requests/sessions.py\", line 530, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/requests/sessions.py\", line 643, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/requests/adapters.py\", line 439, in send\n",
            "    resp = conn.urlopen(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py\", line 650, in urlopen\n",
            "    conn = conn and conn.close()\n",
            "  File \"/usr/lib/python3.8/http/client.py\", line 936, in close\n",
            "    sock.close()   # close it manually... there may be other refs\n",
            "  File \"/usr/lib/python3.8/socket.py\", line 500, in close\n",
            "    self._real_close()\n",
            "  File \"/usr/lib/python3.8/ssl.py\", line 1299, in _real_close\n",
            "    self._sslobj = None\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "java.io.FileNotFoundException: /content/drive/MyDrive/sf-xs.zip (No such file or directory)\n",
            "\tat java.base/java.io.FileInputStream.open0(Native Method)\n",
            "\tat java.base/java.io.FileInputStream.open(FileInputStream.java:219)\n",
            "\tat java.base/java.io.FileInputStream.<init>(FileInputStream.java:157)\n",
            "\tat java.base/java.io.FileInputStream.<init>(FileInputStream.java:112)\n",
            "\tat jdk.jartool/sun.tools.jar.Main.run(Main.java:407)\n",
            "\tat jdk.jartool/sun.tools.jar.Main.main(Main.java:1680)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g6SkCgyhl-g"
      },
      "source": [
        "# Download Code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone \"https://github.com/gmberton/CosPlace\" \n",
        "#!rm -r \"/content/CosPlace\""
      ],
      "metadata": {
        "id": "63SgJ_Y0hwrC",
        "outputId": "f7e19a79-0160-4ce0-8528-0a6182a88201",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CosPlace'...\n",
            "remote: Enumerating objects: 168, done.\u001b[K\n",
            "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 168 (delta 54), reused 53 (delta 32), pack-reused 85\u001b[K\n",
            "Receiving objects: 100% (168/168), 55.31 KiB | 13.83 MiB/s, done.\n",
            "Resolving deltas: 100% (84/84), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiwWzjzSio-h"
      },
      "source": [
        "\n",
        "\n",
        "# Import Code\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORT COSPLACE PYTHON FUNCTIONS \n",
        "import sys, os\n",
        "\n",
        "sys.path.append(\"/content/CosPlace/\")\n",
        "\n",
        "import CosPlace\n",
        "from CosPlace import *\n",
        "\n",
        "if not os.path.isdir(\"content/saved_models\"):\n",
        "  !mkdir saved_models"
      ],
      "metadata": {
        "id": "0qlBEtAcisfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run this to know all the parameters of train"
      ],
      "metadata": {
        "id": "MAR9zkdK44Tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python CosPlace/train.py -h"
      ],
      "metadata": {
        "id": "n-rDgmN_4eka",
        "outputId": "c85f300c-9f84-4480-e278-8ab63422c952",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: train.py\n",
            "       [-h]\n",
            "       [--M M]\n",
            "       [--alpha ALPHA]\n",
            "       [--N N]\n",
            "       [--L L]\n",
            "       [--groups_num GROUPS_NUM]\n",
            "       [--min_images_per_class MIN_IMAGES_PER_CLASS]\n",
            "       [--backbone {vgg16,resnet18,resnet50,resnet101,resnet152}]\n",
            "       [--fc_output_dim FC_OUTPUT_DIM]\n",
            "       [--use_amp16]\n",
            "       [--augmentation_device {cuda,cpu}]\n",
            "       [--batch_size BATCH_SIZE]\n",
            "       [--epochs_num EPOCHS_NUM]\n",
            "       [--iterations_per_epoch ITERATIONS_PER_EPOCH]\n",
            "       [--lr LR]\n",
            "       [--classifiers_lr CLASSIFIERS_LR]\n",
            "       [--brightness BRIGHTNESS]\n",
            "       [--contrast CONTRAST]\n",
            "       [--hue HUE]\n",
            "       [--saturation SATURATION]\n",
            "       [--random_resized_crop RANDOM_RESIZED_CROP]\n",
            "       [--infer_batch_size INFER_BATCH_SIZE]\n",
            "       [--positive_dist_threshold POSITIVE_DIST_THRESHOLD]\n",
            "       [--resume_train RESUME_TRAIN]\n",
            "       [--resume_model RESUME_MODEL]\n",
            "       [--device {cuda,cpu}]\n",
            "       [--seed SEED]\n",
            "       [--num_workers NUM_WORKERS]\n",
            "       [--dataset_folder DATASET_FOLDER]\n",
            "       [--save_dir SAVE_DIR]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help\n",
            "    show this\n",
            "    help\n",
            "    message and\n",
            "    exit\n",
            "  --M M\n",
            "    _ (default:\n",
            "    10)\n",
            "  --alpha ALPHA\n",
            "    _ (default:\n",
            "    30)\n",
            "  --N N\n",
            "    _ (default:\n",
            "    5)\n",
            "  --L L\n",
            "    _ (default:\n",
            "    2)\n",
            "  --groups_num GROUPS_NUM\n",
            "    _ (default:\n",
            "    8)\n",
            "  --min_images_per_class MIN_IMAGES_PER_CLASS\n",
            "    _ (default:\n",
            "    10)\n",
            "  --backbone {vgg16,resnet18,resnet50,resnet101,resnet152}\n",
            "    _ (default:\n",
            "    resnet18)\n",
            "  --fc_output_dim FC_OUTPUT_DIM\n",
            "    Output\n",
            "    dimension\n",
            "    of final\n",
            "    fully\n",
            "    connected\n",
            "    layer\n",
            "    (default:\n",
            "    512)\n",
            "  --use_amp16\n",
            "    use\n",
            "    Automatic\n",
            "    Mixed\n",
            "    Precision\n",
            "    (default:\n",
            "    False)\n",
            "  --augmentation_device {cuda,cpu}\n",
            "    on which\n",
            "    device to\n",
            "    run data au\n",
            "    gmentation\n",
            "    (default:\n",
            "    cuda)\n",
            "  --batch_size BATCH_SIZE\n",
            "    _ (default:\n",
            "    32)\n",
            "  --epochs_num EPOCHS_NUM\n",
            "    _ (default:\n",
            "    50)\n",
            "  --iterations_per_epoch ITERATIONS_PER_EPOCH\n",
            "    _ (default:\n",
            "    10000)\n",
            "  --lr LR\n",
            "    _ (default:\n",
            "    1e-05)\n",
            "  --classifiers_lr CLASSIFIERS_LR\n",
            "    _ (default:\n",
            "    0.01)\n",
            "  --brightness BRIGHTNESS\n",
            "    _ (default:\n",
            "    0.7)\n",
            "  --contrast CONTRAST\n",
            "    _ (default:\n",
            "    0.7)\n",
            "  --hue HUE\n",
            "    _ (default:\n",
            "    0.5)\n",
            "  --saturation SATURATION\n",
            "    _ (default:\n",
            "    0.7)\n",
            "  --random_resized_crop RANDOM_RESIZED_CROP\n",
            "    _ (default:\n",
            "    0.5)\n",
            "  --infer_batch_size INFER_BATCH_SIZE\n",
            "    Batch size\n",
            "    for\n",
            "    inference\n",
            "    (validating\n",
            "    and\n",
            "    testing)\n",
            "    (default:\n",
            "    16)\n",
            "  --positive_dist_threshold POSITIVE_DIST_THRESHOLD\n",
            "    distance in\n",
            "    meters for\n",
            "    a\n",
            "    prediction\n",
            "    to be\n",
            "    considered\n",
            "    a positive\n",
            "    (default:\n",
            "    25)\n",
            "  --resume_train RESUME_TRAIN\n",
            "    path to\n",
            "    checkpoint\n",
            "    to resume,\n",
            "    e.g. logs/.\n",
            "    ../last_che\n",
            "    ckpoint.pth\n",
            "    (default:\n",
            "    None)\n",
            "  --resume_model RESUME_MODEL\n",
            "    path to\n",
            "    model to\n",
            "    resume,\n",
            "    e.g. logs/.\n",
            "    ../best_mod\n",
            "    el.pth\n",
            "    (default:\n",
            "    None)\n",
            "  --device {cuda,cpu}\n",
            "    _ (default:\n",
            "    cuda)\n",
            "  --seed SEED\n",
            "    _ (default:\n",
            "    0)\n",
            "  --num_workers NUM_WORKERS\n",
            "    _ (default:\n",
            "    8)\n",
            "  --dataset_folder DATASET_FOLDER\n",
            "    path of the\n",
            "    folder with\n",
            "    train/val/t\n",
            "    est sets\n",
            "    (default:\n",
            "    None)\n",
            "  --save_dir SAVE_DIR\n",
            "    name of\n",
            "    directory\n",
            "    on which to\n",
            "    save the\n",
            "    logs, under\n",
            "    logs/save_d\n",
            "    ir\n",
            "    (default:\n",
            "    default)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing default train"
      ],
      "metadata": {
        "id": "zK9hTJXqkDht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python CosPlace/train.py --dataset_folder /content/small --save_dir /content/saved_models --groups_num 1 --epochs_num 3"
      ],
      "metadata": {
        "id": "KpGgiE0EioEQ",
        "outputId": "52eda433-0ee5-4f2d-b908-c10a065ae0ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-07 17:10:24   CosPlace/train.py --dataset_folder /content/small --groups_num 1 --epochs_num 1\n",
            "2022-12-07 17:10:24   Arguments: Namespace(L=2, M=10, N=5, alpha=30, augmentation_device='cuda', backbone='resnet18', batch_size=32, brightness=0.7, classifiers_lr=0.01, contrast=0.7, dataset_folder='/content/small', device='cuda', epochs_num=1, fc_output_dim=512, groups_num=1, hue=0.5, infer_batch_size=16, iterations_per_epoch=10000, lr=1e-05, min_images_per_class=10, num_workers=8, positive_dist_threshold=25, random_resized_crop=0.5, resume_model=None, resume_train=None, saturation=0.7, save_dir='default', seed=0, test_set_folder='/content/small/test', train_set_folder='/content/small/train', use_amp16=False, val_set_folder='/content/small/val')\n",
            "2022-12-07 17:10:24   The outputs are being saved in logs/default/2022-12-07_17-10-24\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100% 44.7M/44.7M [00:00<00:00, 79.1MB/s]\n",
            "2022-12-07 17:10:25   Train only layer3 and layer4 of the resnet18, freeze the previous ones\n",
            "2022-12-07 17:10:26   There are 1 GPUs and 2 CPUs.\n",
            "2022-12-07 17:10:27   Cached dataset cache/small_M10_N5_mipc10.torch does not exist, I'll create it now.\n",
            "2022-12-07 17:10:27   Searching training images in /content/small/train\n",
            "2022-12-07 17:10:28   Found 59650 images\n",
            "2022-12-07 17:10:28   For each image, get its UTM east, UTM north and heading from its path\n",
            "2022-12-07 17:10:28   For each image, get class and group to which it belongs\n",
            "2022-12-07 17:10:28   Group together images belonging to the same class\n",
            "2022-12-07 17:10:28   Group together classes belonging to the same group\n",
            "2022-12-07 17:10:28   Using 1 groups\n",
            "2022-12-07 17:10:28   The 1 groups have respectively the following number of classes [5965]\n",
            "2022-12-07 17:10:28   The 1 groups have respectively the following number of images [59650]\n",
            "2022-12-07 17:10:29   Validation set: < val - #q: 7993; #db: 8015 >\n",
            "2022-12-07 17:10:29   Test set: < test - #q: 1000; #db: 27191 >\n",
            "2022-12-07 17:10:29   Start training ...\n",
            "2022-12-07 17:10:29   There are 5965 classes for the first group, each epoch has 10000 iterations with batch_size 32, therefore the model sees each class (on average) 53.6 times per epoch\n",
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "100%|███████████████████████████████████████████████████████| 10000/10000 [1:16:12<00:00,  2.19it/s]\n",
            "2022-12-07 18:26:42   Epoch 00 in 1:16:12, loss = 7.9505\n",
            "2022-12-07 18:26:42   Extracting database descriptors for evaluation/testing\n",
            "100%|█████████████████████████████████████████████████████████████| 501/501 [01:09<00:00,  7.24it/s]\n",
            "2022-12-07 18:27:51   Extracting queries descriptors for evaluation/testing using batch size 1\n",
            "100%|███████████████████████████████████████████████████████████| 7993/7993 [01:52<00:00, 71.35it/s]\n",
            "2022-12-07 18:29:43   Calculating recalls\n",
            "2022-12-07 18:29:44   Epoch 00 in 1:19:15, < val - #q: 7993; #db: 8015 >: R@1: 78.6, R@5: 88.0\n",
            "2022-12-07 18:29:45   Trained for 01 epochs, in total in 1:19:20\n",
            "2022-12-07 18:29:45   Now testing on the test set: < test - #q: 1000; #db: 27191 >\n",
            "2022-12-07 18:29:45   Extracting database descriptors for evaluation/testing\n",
            "100%|███████████████████████████████████████████████████████████| 1700/1700 [03:47<00:00,  7.47it/s]\n",
            "2022-12-07 18:33:32   Extracting queries descriptors for evaluation/testing using batch size 1\n",
            "100%|███████████████████████████████████████████████████████████| 1000/1000 [00:16<00:00, 61.46it/s]\n",
            "2022-12-07 18:33:49   Calculating recalls\n",
            "2022-12-07 18:33:49   < test - #q: 1000; #db: 27191 >: R@1: 44.9, R@5: 61.0, R@10: 65.5, R@20: 71.2\n",
            "2022-12-07 18:33:49   Experiment finished (without any errors)\n"
          ]
        }
      ]
    }
  ]
}